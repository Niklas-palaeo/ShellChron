SD_bin = sd(D47, na.rm = TRUE), # Calculate stdevs per sample bin
SD_ext = first(D47_SD), # External standard deviation (reported or based on check STD)
binsd = if(is.na(SD_bin)){first(SD_ext)}else{max(SD_bin, SD_ext)}, # Find largest standard deviation
Nbin = n(), # Calculate the number of modelled values, excluding NA's
binse = binsd / sqrt(Nbin), # Calculate the standard error
binCL95 = qt(0.95, Nbin) * binse, # Calculate the 95% confidence level
param49_binmean = mean(param49, na.rm = TRUE), # Propagate param49 mean per sample
param49_binsd = sd(param49, na.rm = TRUE), # Propagate param49 SD per sample
Machine = first(Machine)
) %>%
ungroup()
# write.csv(D47stats_Machinebin, "C:/Users/Niels/Dropbox//Research//postdoc//UNBIAS//Clumped Temperature Calibration/Aragonite_dataset_stats_by_Machine.csv")
# Now combine sample groups from different machines into one bin
D47stats <- D47stats_Machinebin %>%
group_by(ID) %>%
summarize(
Temp = mean(Temp, na.rm = TRUE),
Temp_SD = first(Temp_SD),
Analysis = first(Analysis),
type = first(type),
sample = first(sample),
D47_mean = binmeans(x = D47_binmean, x_sd = binsd, n = Nbin, output = "mean"),  # Calculate means per sample
sd = binmeans(x = D47_binmean, x_sd = binsd, n = Nbin, output = "SD"), # Calculate stdevs per sample bin
N = sum(Nbin, na.rm = TRUE), # Calculate the number of modelled values, excluding NA's
se = sd / sqrt(N), # Calculate the standard error
CL95 = qt(0.95, N) * se, # Calculate the 95% confidence level
param49_mean = binmeans(x = param49_binmean, x_sd = param49_binsd, n = Nbin, output = "mean"),  # Calculate means per sample
param49_sd = binmeans(x = param49_binmean, x_sd = param49_binsd, n = Nbin, output = "SD"), # Calculate stdevs per sample bin
param49_se = param49_sd / sqrt(N),
param49_CL95 = qt(0.95, N) * param49_se
)
# write.csv(D47stats, "C:/Users/Niels/Dropbox//Research//postdoc//UNBIAS//Clumped Temperature Calibration/Aragonite_dataset_stats_grouped.csv")
# Summarize stats for Arctica islandica samples per specimen
# First propagate uncertainties on bins per specimen, keeping data from different machines separate
Aisstats_Machinebin <- dat[which(dat$D47_outlier != TRUE & dat$Analysis == "this study"), ] %>% # Summarize D47 statistics
group_by(Specimen, Machine) %>%
summarize(
sample = first(sample),
Temp = mean(Temp, na.rm = TRUE),
Temp_SD = first(Temp_SD),
Analysis = first(Analysis),
type = first(type),
sample = first(sample),
D47_binmean = mean(D47, na.rm = TRUE),  # Calculate means per sample
SD_bin = sd(D47, na.rm = TRUE), # Calculate stdevs per sample bin
SD_ext = first(D47_SD), # External standard deviation (reported or based on check STD)
binsd = max(SD_bin, SD_ext), # Find largest standard
Nbin = n(), # Calculate the number of modelled values, excluding NA's
binse = binsd / sqrt(Nbin), # Calculate the standard error
binCL95 = qt(0.95, Nbin) * binse, # Calculate the 95% confidence level
param49_binmean = mean(param49, na.rm = TRUE), # Propagate param49 mean per sample
param49_binsd = sd(param49, na.rm = TRUE), # Propagate param49 SD per sample
Machine = first(Machine)
) %>%
ungroup()
# write.csv(Aisstats_Machinebin, "C:/Users/Niels/Dropbox//Research//postdoc//UNBIAS//Clumped Temperature Calibration/Ais_dataset_stats_by_Machine.csv")
# Now combine sample groups from different machines into one bin
Aisstats <- Aisstats_Machinebin %>%
group_by(Specimen) %>%
summarize(
Temp = mean(Temp, na.rm = TRUE),
Temp_SD = first(Temp_SD),
Analysis = first(Analysis),
type = first(type),
sample = first(sample),
D47_mean = binmeans(x = D47_binmean, x_sd = binsd, n = Nbin, output = "mean"),  # Calculate means per sample
sd = binmeans(x = D47_binmean, x_sd = binsd, n = Nbin, output = "SD"), # Calculate stdevs per sample bin
N = sum(Nbin, na.rm = TRUE), # Calculate the number of modelled values, excluding NA's
se = sd / sqrt(N), # Calculate the standard error
CL95 = qt(0.95, N) * se, # Calculate the 95% confidence level
param49_mean = binmeans(x = param49_binmean, x_sd = param49_binsd, n = Nbin, output = "mean"),  # Calculate means per sample
param49_sd = binmeans(x = param49_binmean, x_sd = param49_binsd, n = Nbin, output = "SD"), # Calculate stdevs per sample bin
param49_se = param49_sd / sqrt(N),
param49_CL95 = qt(0.95, N) * param49_se
)
# write.csv(Aisstats, "C:/Users/Niels/Dropbox//Research//postdoc//UNBIAS//Clumped Temperature Calibration/Ais_dataset_stats_grouped.csv")
# Update dat with new propagated SDs for regressions
newSDs <- D47stats[which(D47stats$Analysis == "this study"), which(colnames(D47stats) %in% c("ID", "sd"))]
dat$D47_SD[which(!is.na(match(dat$ID, newSDs$ID)))] <- newSDs$sd[match(dat$ID, newSDs$ID)[which(!is.na(match(dat$ID, newSDs$ID)))]]
# Monte Carlo sample from individual aliquot distributions for violin plots and polynomial regression including errors on D47
Nsim <- 10 ^ 4
violin_data <- data.frame(sample = rep(dat$sample, Nsim),
type = rep(dat$type, Nsim),
Analysis = rep(dat$Analysis, Nsim),
ID = rep(dat$ID, Nsim),
Specimen = rep(dat$Specimen, Nsim),
Temp = rep(dat$Temp, Nsim),
Temp_sampled = rnorm(Nsim * length(dat$Temp), dat$Temp, dat$Temp_SD),
D47 = rnorm(Nsim * length(dat$D47), dat$D47, dat$D47_SD),
param49 = rnorm(Nsim * length(dat$param49), dat$param49, dat$param49_sd),
outlier = rep(dat$D47_outlier, Nsim)
)
violin_data$x <- 10 ^ 6 / (violin_data$Temp_sampled + 273.15) ^ 2 # Create 10^6/T^2 vector
# ------------------------Pairwise comparisons----------------------------------
Aisdata <- dat[which(dat$D47_outlier != TRUE & dat$Analysis == "this study"), ] # Isolate Arctica islandica data from this study
Ais_temp_aov <- aov(D47 ~ ID, data = Aisdata) # Conduct one-way ANOVA on temperature bins
capture.output(summary(Ais_temp_aov), file = "C:/Users/Niels/Dropbox//Research//postdoc//UNBIAS//Clumped Temperature Calibration/Ais_Pairwise_comp_temp_summary.txt") # Print summary of ANOVA
TukeyHSD(Ais_temp_aov) # Print results of Tukey multiple pairwise-comparisons (post-hoc Tukey's test) on temperature bins
# write.csv(TukeyHSD(Ais_temp_aov)$ID, "C:/Users/Niels/Dropbox//Research//postdoc//UNBIAS//Clumped Temperature Calibration/Ais_Pairwise_comp_temp.csv") # Export summary of Tukey multiple pairwise-comparisons
Ais_spec1_aov <- aov(D47 ~ Specimen, data = Aisdata[which(Aisdata$Temp == 1.1), ]) # Conduct one-way ANOVA on Specimen bins within the 1 degree temperature treatment
capture.output(summary(Ais_spec1_aov), file = "C:/Users/Niels/Dropbox//Research//postdoc//UNBIAS//Clumped Temperature Calibration/Ais_Pairwise_comp_spec1_summary.txt") # Print summary of ANOVA
TukeyHSD(Ais_spec1_aov) # Print results of Tukey multiple pairwise-comparisons (post-hoc Tukey's test) on specimen bins
# write.csv(TukeyHSD(Ais_spec1_aov)$Specimen, "C:/Users/Niels/Dropbox//Research//postdoc//UNBIAS//Clumped Temperature Calibration/Ais_Pairwise_comp_spec1.csv") # Export summary of Tukey multiple pairwise-comparisons
Ais_spec18_aov <- aov(D47 ~ Specimen, data = Aisdata[which(Aisdata$Temp == 18.0), ]) # Conduct one-way ANOVA on Specimen bins within the 18 degree temperature treatment
capture.output(summary(Ais_spec18_aov), file = "C:/Users/Niels/Dropbox//Research//postdoc//UNBIAS//Clumped Temperature Calibration/Ais_Pairwise_comp_spec18_summary.txt") # Print summary of ANOVA
TukeyHSD(Ais_spec18_aov) # Print results of Tukey multiple pairwise-comparisons (post-hoc Tukey's test) on specimen bins
# write.csv(TukeyHSD(Ais_spec18_aov)$Specimen, "C:/Users/Niels/Dropbox//Research//postdoc//UNBIAS//Clumped Temperature Calibration/Ais_Pairwise_comp_spec18.csv") # Export summary of Tukey multiple pairwise-comparisons
# ----------------------------Regressions---------------------------------------
# Start with simple lm
D47m <- lm(D47 ~ I(10^6 / (Temp + 273.15) ^ 2), data = dat, subset = which(dat$D47_outlier == FALSE))
newdat <- data.frame(Temp = seq(0, 1000, 0.1))
D47m_pred <- predict.lm(D47m, newdata = newdat, se.fit = TRUE, interval = "confidence", level = 0.95)
D47m_result <- cbind(10^6 / (newdat + 273.15) ^2, D47m_pred$fit)
# Repeat excluding the high-T datapoints (MÃ¼ller et al., 2017)
D47m_lowT <- lm(D47 ~ I(10^6 / (Temp + 273.15) ^ 2), data = dat, subset = which(dat$D47_outlier == FALSE & dat$Analysis != "Muller17"))
newdat_lowT <- data.frame(Temp = seq(0, 100, 0.1))
D47m_lowT_pred <- predict.lm(D47m_lowT, newdata = newdat_lowT, se.fit = TRUE, interval = "confidence", level = 0.95)
D47m_lowT_result <- cbind(10^6 / (newdat_lowT + 273.15) ^2, D47m_lowT_pred$fit)
# Then try linear York regression on both
D47m_York <- bfsl(x = 10^6 / (dat$Temp[dat$D47_outlier == FALSE] + 273.15) ^ 2,
y = dat$D47[dat$D47_outlier == FALSE],
sd_x = abs(10^6 / (dat$Temp[dat$D47_outlier == FALSE] + dat$Temp_SD[dat$D47_outlier == FALSE] + 273.15) ^ 2 - 10^6 / (dat$Temp[dat$D47_outlier == FALSE] - dat$Temp_SD[dat$D47_outlier == FALSE] + 273.15) ^ 2) / 2,
sd_y = dat$D47_SD[dat$D47_outlier == FALSE],
r = 0)
newdat_York <- data.frame(x = 10 ^6 / (seq(0, 1000, 0.1) + 273.15) ^ 2)
D47m_York_pred <- predict(D47m_York, newdata = newdat_York, se.fit = TRUE, interval = "confidence", level = 0.95)
D47m_York_result <- cbind(newdat_York, D47m_York_pred$fit)
D47m_lowT_York <- bfsl(x = 10^6 / (dat$Temp[dat$D47_outlier == FALSE & dat$Analysis != "Muller17"] + 273.15) ^ 2,
y = dat$D47[dat$D47_outlier == FALSE & dat$Analysis != "Muller17"],
sd_x = abs(10^6 / (dat$Temp[dat$D47_outlier == FALSE & dat$Analysis != "Muller17"] + dat$Temp_SD[dat$D47_outlier == FALSE & dat$Analysis != "Muller17"] + 273.15) ^ 2 - 10^6 / (dat$Temp[dat$D47_outlier == FALSE & dat$Analysis != "Muller17"] - dat$Temp_SD[dat$D47_outlier == FALSE & dat$Analysis != "Muller17"] + 273.15) ^ 2) / 2,
sd_y = dat$D47_SD[dat$D47_outlier == FALSE & dat$Analysis != "Muller17"],
r = 0)
newdat_lowT_York <- data.frame(x = 10 ^6 / (seq(0, 100, 0.1) + 273.15) ^ 2)
D47m_lowT_York_pred <- predict(D47m_lowT_York, newdata = newdat_lowT_York, se.fit = TRUE, interval = "confidence", level = 0.95)
D47m_lowT_York_result <- cbind(newdat_lowT_York, D47m_lowT_York_pred$fit)
# Also calculate linear York regression on A. islandica and all bivalve data
Ais_York <- bfsl(x = 10^6 / (dat$Temp[dat$D47_outlier == FALSE & (dat$Analysis == "this study" | dat$ID == "A.islandica")] + 273.15) ^ 2,
y = dat$D47[dat$D47_outlier == FALSE & (dat$Analysis == "this study" | dat$ID == "A.islandica")],
sd_x = abs(10^6 / (dat$Temp[dat$D47_outlier == FALSE & (dat$Analysis == "this study" | dat$ID == "A.islandica")] + dat$Temp_SD[dat$D47_outlier == FALSE & (dat$Analysis == "this study" | dat$ID == "A.islandica")] + 273.15) ^ 2 - 10^6 / (dat$Temp[dat$D47_outlier == FALSE & (dat$Analysis == "this study" | dat$ID == "A.islandica")] - dat$Temp_SD[dat$D47_outlier == FALSE & (dat$Analysis == "this study" | dat$ID == "A.islandica")] + 273.15) ^ 2) / 2,
sd_y = dat$D47_SD[dat$D47_outlier == FALSE & (dat$Analysis == "this study" | dat$ID == "A.islandica")],
r = 0)
Ais_York_pred <- predict(Ais_York, newdata = newdat_lowT_York, se.fit = TRUE, interval = "confidence", level = 0.95)
Ais_York_result <- cbind(newdat_lowT_York, Ais_York_pred$fit)
Ais_York_MOTU <- bfsl(x = 10^6 / (dat$Temp[dat$D47_outlier == FALSE & ((dat$Analysis == "this study" & dat$Machine == "MOTU") | dat$ID == "A.islandica")] + 273.15) ^ 2,
y = dat$D47[dat$D47_outlier == FALSE & ((dat$Analysis == "this study" & dat$Machine == "MOTU") | dat$ID == "A.islandica")],
sd_x = abs(10^6 / (dat$Temp[dat$D47_outlier == FALSE & ((dat$Analysis == "this study" & dat$Machine == "MOTU") | dat$ID == "A.islandica")] + dat$Temp_SD[dat$D47_outlier == FALSE & ((dat$Analysis == "this study" & dat$Machine == "MOTU") | dat$ID == "A.islandica")] + 273.15) ^ 2 - 10^6 / (dat$Temp[dat$D47_outlier == FALSE & ((dat$Analysis == "this study" & dat$Machine == "MOTU") | dat$ID == "A.islandica")] - dat$Temp_SD[dat$D47_outlier == FALSE & ((dat$Analysis == "this study" & dat$Machine == "MOTU") | dat$ID == "A.islandica")] + 273.15) ^ 2) / 2,
sd_y = dat$D47_SD[dat$D47_outlier == FALSE & ((dat$Analysis == "this study" & dat$Machine == "MOTU") | dat$ID == "A.islandica")],
r = 0)
Ais_York_MOTU_pred <- predict(Ais_York_MOTU, newdata = newdat_lowT_York, se.fit = TRUE, interval = "confidence", level = 0.95)
Ais_York_MOTU_result <- cbind(newdat_lowT_York, Ais_York_MOTU_pred$fit)
Ais_York_PACMAN <- bfsl(x = 10^6 / (dat$Temp[dat$D47_outlier == FALSE & ((dat$Analysis == "this study" & dat$Machine == "PACMAN") | dat$ID == "A.islandica")] + 273.15) ^ 2,
y = dat$D47[dat$D47_outlier == FALSE & ((dat$Analysis == "this study" & dat$Machine == "PACMAN") | dat$ID == "A.islandica")],
sd_x = abs(10^6 / (dat$Temp[dat$D47_outlier == FALSE & ((dat$Analysis == "this study" & dat$Machine == "PACMAN") | dat$ID == "A.islandica")] + dat$Temp_SD[dat$D47_outlier == FALSE & ((dat$Analysis == "this study" & dat$Machine == "PACMAN") | dat$ID == "A.islandica")] + 273.15) ^ 2 - 10^6 / (dat$Temp[dat$D47_outlier == FALSE & ((dat$Analysis == "this study" & dat$Machine == "PACMAN") | dat$ID == "A.islandica")] - dat$Temp_SD[dat$D47_outlier == FALSE & ((dat$Analysis == "this study" & dat$Machine == "PACMAN") | dat$ID == "A.islandica")] + 273.15) ^ 2) / 2,
sd_y = dat$D47_SD[dat$D47_outlier == FALSE & ((dat$Analysis == "this study" & dat$Machine == "PACMAN") | dat$ID == "A.islandica")],
r = 0)
Ais_York_PACMAN_pred <- predict(Ais_York_PACMAN, newdata = newdat_lowT_York, se.fit = TRUE, interval = "confidence", level = 0.95)
Ais_York_PACMAN_result <- cbind(newdat_lowT_York, Ais_York_PACMAN_pred$fit)
Mollusk_York <- bfsl(x = 10^6 / (dat$Temp[dat$D47_outlier == FALSE & (dat$Analysis == "this study" | dat$ID == "A.islandica" | dat$Analysis == "Caldarescu21")] + 273.15) ^ 2,
y = dat$D47[dat$D47_outlier == FALSE & (dat$Analysis == "this study" | dat$ID == "A.islandica" | dat$Analysis == "Caldarescu21")],
sd_x = abs(10^6 / (dat$Temp[dat$D47_outlier == FALSE & (dat$Analysis == "this study" | dat$ID == "A.islandica" | dat$Analysis == "Caldarescu21")] + dat$Temp_SD[dat$D47_outlier == FALSE & (dat$Analysis == "this study" | dat$ID == "A.islandica" | dat$Analysis == "Caldarescu21")] + 273.15) ^ 2 - 10^6 / (dat$Temp[dat$D47_outlier == FALSE & (dat$Analysis == "this study" | dat$ID == "A.islandica" | dat$Analysis == "Caldarescu21")] - dat$Temp_SD[dat$D47_outlier == FALSE & (dat$Analysis == "this study" | dat$ID == "A.islandica" | dat$Analysis == "Caldarescu21")] + 273.15) ^ 2) / 2,
sd_y = dat$D47_SD[dat$D47_outlier == FALSE & (dat$Analysis == "this study" | dat$ID == "A.islandica" | dat$Analysis == "Caldarescu21")],
r = 0)
Mollusk_York_pred <- predict(Mollusk_York, newdata = newdat_lowT_York, se.fit = TRUE, interval = "confidence", level = 0.95)
Mollusk_York_result <- cbind(newdat_lowT_York, Mollusk_York_pred$fit)
Mollusk_York_MOTU <- bfsl(x = 10^6 / (dat$Temp[dat$D47_outlier == FALSE & ((dat$Analysis == "this study" & dat$Machine == "MOTU") | dat$ID == "A.islandica" | dat$Analysis == "Caldarescu21")] + 273.15) ^ 2,
y = dat$D47[dat$D47_outlier == FALSE & ((dat$Analysis == "this study" & dat$Machine == "MOTU") | dat$ID == "A.islandica" | dat$Analysis == "Caldarescu21")],
sd_x = abs(10^6 / (dat$Temp[dat$D47_outlier == FALSE & ((dat$Analysis == "this study" & dat$Machine == "MOTU") | dat$ID == "A.islandica" | dat$Analysis == "Caldarescu21")] + dat$Temp_SD[dat$D47_outlier == FALSE & ((dat$Analysis == "this study" & dat$Machine == "MOTU") | dat$ID == "A.islandica" | dat$Analysis == "Caldarescu21")] + 273.15) ^ 2 - 10^6 / (dat$Temp[dat$D47_outlier == FALSE & ((dat$Analysis == "this study" & dat$Machine == "MOTU") | dat$ID == "A.islandica" | dat$Analysis == "Caldarescu21")] - dat$Temp_SD[dat$D47_outlier == FALSE & ((dat$Analysis == "this study" & dat$Machine == "MOTU") | dat$ID == "A.islandica" | dat$Analysis == "Caldarescu21")] + 273.15) ^ 2) / 2,
sd_y = dat$D47_SD[dat$D47_outlier == FALSE & ((dat$Analysis == "this study" & dat$Machine == "MOTU") | dat$ID == "A.islandica" | dat$Analysis == "Caldarescu21")],
r = 0)
Mollusk_York_MOTU_pred <- predict(Mollusk_York_MOTU, newdata = newdat_lowT_York, se.fit = TRUE, interval = "confidence", level = 0.95)
Mollusk_York_MOTU_result <- cbind(newdat_lowT_York, Mollusk_York_MOTU_pred$fit)
Mollusk_York_PACMAN <- bfsl(x = 10^6 / (dat$Temp[dat$D47_outlier == FALSE & ((dat$Analysis == "this study" & dat$Machine == "PACMAN") | dat$ID == "A.islandica" | dat$Analysis == "Caldarescu21")] + 273.15) ^ 2,
y = dat$D47[dat$D47_outlier == FALSE & ((dat$Analysis == "this study" & dat$Machine == "PACMAN") | dat$ID == "A.islandica" | dat$Analysis == "Caldarescu21")],
sd_x = abs(10^6 / (dat$Temp[dat$D47_outlier == FALSE & ((dat$Analysis == "this study" & dat$Machine == "PACMAN") | dat$ID == "A.islandica" | dat$Analysis == "Caldarescu21")] + dat$Temp_SD[dat$D47_outlier == FALSE & ((dat$Analysis == "this study" & dat$Machine == "PACMAN") | dat$ID == "A.islandica" | dat$Analysis == "Caldarescu21")] + 273.15) ^ 2 - 10^6 / (dat$Temp[dat$D47_outlier == FALSE & ((dat$Analysis == "this study" & dat$Machine == "PACMAN") | dat$ID == "A.islandica" | dat$Analysis == "Caldarescu21")] - dat$Temp_SD[dat$D47_outlier == FALSE & ((dat$Analysis == "this study" & dat$Machine == "PACMAN") | dat$ID == "A.islandica" | dat$Analysis == "Caldarescu21")] + 273.15) ^ 2) / 2,
sd_y = dat$D47_SD[dat$D47_outlier == FALSE & ((dat$Analysis == "this study" & dat$Machine == "PACMAN") | dat$ID == "A.islandica" | dat$Analysis == "Caldarescu21")],
r = 0)
Mollusk_York_PACMAN_pred <- predict(Mollusk_York_PACMAN, newdata = newdat_lowT_York, se.fit = TRUE, interval = "confidence", level = 0.95)
Mollusk_York_PACMAN_result <- cbind(newdat_lowT_York, Mollusk_York_PACMAN_pred$fit)
# Third order polynomial regression following MÃ¼ller et al., 2019 and Jautzy et al., 2020
D47m_poly <- lm(D47 ~ poly(I(10^6 / (Temp + 273.15) ^ 2), 3), data = dat, subset = which(dat$D47_outlier == FALSE))
D47m_poly_pred <- predict.lm(D47m_poly, newdata = newdat, se.fit = TRUE, interval = "confidence", level = 0.95)
D47m_poly_result <- cbind(10^6 / (newdat + 273.15) ^2, D47m_poly_pred$fit)
# Polynomial regression with errors on D47 and Temp using MC simulation
D47m_poly_MC <- lm(D47 ~ poly(x, 3), data = violin_data, subset = which(violin_data$outlier == FALSE))
D47m_poly_MC_pred <- predict.lm(D47m_poly_MC, newdata = newdat_York, se.fit = TRUE, interval = "confidence", level = 0.95)
D47m_poly_MC_result <- cbind(newdat_York, D47m_poly_MC_pred$fit)
# Readjust 95% CL calculations for actual degrees of freedom
D47m_poly_MC_result$lwr <- D47m_poly_MC_result$fit - (D47m_poly_MC_result$fit - D47m_poly_MC_result$lwr) * sqrt(D47m_poly_MC$df.residual) / sqrt(length(violin_data$D47) / Nsim - 4)
D47m_poly_MC_result$upr <- D47m_poly_MC_result$fit + (D47m_poly_MC_result$upr - D47m_poly_MC_result$fit) * sqrt(D47m_poly_MC$df.residual) / sqrt(length(violin_data$D47) / Nsim - 4)
# -----------------------Add Preexisting calibrations---------------------------
# Add dummy data to plot Anderson calibration
Anderson21 <- data.frame(Temp = 10 ^ 6 / (seq(0, 1000, 0.1) + 273.15) ^ 2,
D47 = 0.0391 * 10 ^ 6 / (seq(0, 1000, 0.1) + 273.15) ^ 2 + 0.154) # Latest Anderson calibration
# Add dummy data to plot Meinicke calibration
MeinickeICDES <- data.frame(Temp = 10 ^ 6 / (seq(0, 1000, 0.1) + 273.15) ^ 2,
D47 = 0.0397 * 10 ^ 6 / (seq(0, 1000, 0.1) + 273.15) ^ 2 + 0.1518) # Recalculated Meinicke calibration
# Add theoretical calcite and aragonite calibration lines by Guo et al. 2009 recalculated to the I-CDES scale
Guo09 <- data.frame(Temp = 10 ^ 6 / (seq(0, 1000, 0.1) + 273.15) ^ 2,
D47_cc_original = -3.33040 * 10 ^ 9 / (seq(0, 1000, 0.1) + 273.15) ^ 4 + 2.32415 * 10 ^ 7 / (seq(0, 1000, 0.1) + 273.15) ^ 3 - 2.91282 * 10 ^ 3 / (seq(0, 1000, 0.1) + 273.15) ^ 2 - 5.54042 / (seq(0, 1000, 0.1) + 273.15), # Original formula published by Guo et al., 2009
D47_ar_original = -3.43068 * 10 ^ 9 / (seq(0, 1000, 0.1) + 273.15) ^ 4 + 2.35766 * 10 ^ 7 / (seq(0, 1000, 0.1) + 273.15) ^ 3 - 8.06003 * 10 ^ 2 / (seq(0, 1000, 0.1) + 273.15) ^ 2 - 6.90300 / (seq(0, 1000, 0.1) + 273.15), # Original formula published by Guo et al., 2009
D47_cc_CDES25 = -3.33040 * 10 ^ 9 / (seq(0, 1000, 0.1) + 273.15) ^ 4 + 2.32415 * 10 ^ 7 / (seq(0, 1000, 0.1) + 273.15) ^ 3 - 2.91282 * 10 ^ 3 / (seq(0, 1000, 0.1) + 273.15) ^ 2 - 5.54042 / (seq(0, 1000, 0.1) + 273.15) + 0.23252 + 0.268 - 0.232, # Corrected to CDES reference fram by updating the D47-D63 fractionation factor from 0.232 to 0.268
D47_ar_CDES25 = -3.43068 * 10 ^ 9 / (seq(0, 1000, 0.1) + 273.15) ^ 4 + 2.35766 * 10 ^ 7 / (seq(0, 1000, 0.1) + 273.15) ^ 3 - 8.06003 * 10 ^ 2 / (seq(0, 1000, 0.1) + 273.15) ^ 2 - 6.90300 / (seq(0, 1000, 0.1) + 273.15) + 0.22893 + 0.268 - 0.232, # Corrected to CDES reference fram by updating the D47-D63 fractionation factor from 0.232 to 0.268
D47_cc_CDES90 = -3.33040 * 10 ^ 9 / (seq(0, 1000, 0.1) + 273.15) ^ 4 + 2.32415 * 10 ^ 7 / (seq(0, 1000, 0.1) + 273.15) ^ 3 - 2.91282 * 10 ^ 3 / (seq(0, 1000, 0.1) + 273.15) ^ 2 - 5.54042 / (seq(0, 1000, 0.1) + 273.15) + 0.23252 + 0.268 - 0.232 - 0.088, # Bring to CDES90 by applying the 25-70 degrees acid fractionation factor by Petersen et al., 2019
D47_ar_CDES90 = -3.43068 * 10 ^ 9 / (seq(0, 1000, 0.1) + 273.15) ^ 4 + 2.35766 * 10 ^ 7 / (seq(0, 1000, 0.1) + 273.15) ^ 3 - 8.06003 * 10 ^ 2 / (seq(0, 1000, 0.1) + 273.15) ^ 2 - 6.90300 / (seq(0, 1000, 0.1) + 273.15) + 0.22893 + 0.268 - 0.232 - 0.088, # Bring to CDES90 by applying the 25-70 degrees acid fractionation factor by Petersen et al., 2019
D47_cc_CDES90_corr = (-3.33040 * 10 ^ 9 / (seq(0, 1000, 0.1) + 273.15) ^ 4 + 2.32415 * 10 ^ 7 / (seq(0, 1000, 0.1) + 273.15) ^ 3 - 2.91282 * 10 ^ 3 / (seq(0, 1000, 0.1) + 273.15) ^ 2 - 5.54042 / (seq(0, 1000, 0.1) + 273.15) + 0.23252 + 0.268 - 0.232 - 0.088) * 1.035, # Correct for D47-dependent D47-D63 fractionation factor of 35 ppm/per mille found by Guo et al. 2009 (and implemented in Jautzy et al., 2020)
D47_ar_CDES90_corr = (-3.43068 * 10 ^ 9 / (seq(0, 1000, 0.1) + 273.15) ^ 4 + 2.35766 * 10 ^ 7 / (seq(0, 1000, 0.1) + 273.15) ^ 3 - 8.06003 * 10 ^ 2 / (seq(0, 1000, 0.1) + 273.15) ^ 2 - 6.90300 / (seq(0, 1000, 0.1) + 273.15) + 0.22893 + 0.268 - 0.232 - 0.088) * 1.035 # Correct for D47-dependent D47-D63 fractionation factor of 35 ppm/per mille found by Guo et al. 2009 (and implemented in Jautzy et al., 2020)
)
# Values used to put Guo et al. 2009 data on the I-CDES scale
# ETH-1 - Iso A (Meckler et al., 2014) = 600 degr with D47 of 0.2052
# ETH-2 - Iso B (Meckler et al., 2014) = 600 degr with D47 of 0.2085
# ETH-3 - Iso C (Meckler et al., 2014) = ~20 degr with D47 of 0.6132
ETH1_GuoCDES <- Guo09$D47_ar_CDES25[which(Guo09$Temp == 10 ^ 6 / (600 + 273.15) ^ 2)]
ETH3_GuoCDES <- Guo09$D47_ar_CDES25[which(Guo09$Temp == 10 ^ 6 / (20 + 273.15) ^ 2)]
ETH1_ICDES <- 0.2052
ETH3_ICDES <- 0.6132
# Place on I-CDES scale
Guo09$D47_cc_ICDES <- (Guo09$D47_cc_CDES25 - ETH1_GuoCDES) * (ETH1_ICDES-ETH3_ICDES)/(ETH1_GuoCDES-ETH3_GuoCDES) + ETH1_ICDES # Use linear correction through ETH1 and ETH3 values to transform to I-CDES scale
Guo09$D47_ar_ICDES <- (Guo09$D47_ar_CDES25 - ETH1_GuoCDES) * (ETH1_ICDES-ETH3_ICDES)/(ETH1_GuoCDES-ETH3_GuoCDES) + ETH1_ICDES # Use linear correction through ETH1 and ETH3 values to transform to I-CDES scale
Guo09$D47_cc <- Guo09$D47_cc_ICDES * 1.035 # Correct for D47-dependent D47-D63 fractionation factor of 35 ppm/per mille found by Guo et al. 2009 (and implemented in Jautzy et al., 2020)
Guo09$D47_ar <- Guo09$D47_ar_ICDES * 1.035 # Correct for D47-dependent D47-D63 fractionation factor of 35 ppm/per mille found by Guo et al. 2009 (and implemented in Jautzy et al., 2020)
View(D47m_York_result)
View(D47m_lowT_York_result)
D47m_lowT_York_result$Diff_D47m_York <- D47m_lowT_York_result$fit - D47m_York_result$fit[1:1001]
plot(D47m_lowT_York_result$Diff_D47m_York)
D47m_lowT_York_result$err <- D47m_lowT_York_result$fit - D47m_lowT_York_result$lwr
D47m_York_result$err <- D47m_York_result$fit - D47m_York_result$lwr
View(Temperature_offset)
View(Temperature_offset)
Temperature_offset(0.68755, 0.0052, 0, 0.0449, 0.0291)
Temperature_offset(0.68755, 0.005, 0, 0.0449, 0.0291)
Temperature_offset(0.6925, 0.005, 0, 0.0449, 0.0291)
Temperature_offset(0.6925, 0.005, 0, 0.0450, 0.0896)
Temperature_offset(0.4128477, 0.010, 0, 0.0450, 0.0896)
Temperature_offset(0.4128477, 0.00399, 0, 0.0450, 0.0896)
# -------------------------Regression residuals---------------------------------
# Extract D47 values and their errors for temperatures of all samples
newdat_York <- data.frame(x = 10 ^6 / (dat$Temp + 273.15) ^ 2)
dat$D47res_York <- dat$D47 - predict(D47m_York, newdata = newdat_York, se.fit = TRUE, interval = "none", level = 0.95)$fit
dat$D47res_lowT_York <- dat$D47 - predict(D47m_lowT_York, newdata = newdat_York, se.fit = TRUE, interval = "none", level = 0.95)$fit
dat$D47res_poly <- dat$D47 - predict(D47m_poly_MC, newdata = newdat_York, se.fit = TRUE, interval = "none", level = 0.95)$fit
dat$D47res_Anderson <- dat$D47 - (0.0391 * 10 ^ 6 / (dat$Temp + 273.15) ^ 2 + 0.154)
dat$D47res_Meinicke <- dat$D47 - (0.0397 * 10 ^ 6 / (dat$Temp + 273.15) ^ 2 + 0.1518)
dat$D47res_Guo_cc <- dat$D47 - (((-3.33040 * 10 ^ 9 / (dat$Temp + 273.15) ^ 4 + 2.32415 * 10 ^ 7 / (dat$Temp + 273.15) ^ 3 - 2.91282 * 10 ^ 3 / (dat$Temp + 273.15) ^ 2 - 5.54042 / (dat$Temp + 273.15) + 0.23252 + 0.268 - 0.232) - ETH1_GuoCDES) * (ETH1_ICDES-ETH3_ICDES)/(ETH1_GuoCDES-ETH3_GuoCDES) + ETH1_ICDES) * 1.035
dat$D47res_Guo_ar <- dat$D47 - (((-3.43068 * 10 ^ 9 / (dat$Temp + 273.15) ^ 4 + 2.35766 * 10 ^ 7 / (dat$Temp + 273.15) ^ 3 - 8.06003 * 10 ^ 2 / (dat$Temp + 273.15) ^ 2 - 6.90300 / (dat$Temp + 273.15) + 0.22893 + 0.268 - 0.232) - ETH1_GuoCDES) * (ETH1_ICDES-ETH3_ICDES)/(ETH1_GuoCDES-ETH3_GuoCDES) + ETH1_ICDES) * 1.035
newdat_York <- data.frame(x = 10 ^6 / (D47stats$Temp + 273.15) ^ 2)
D47stats$D47res_York <- D47stats$D47_mean - predict(D47m_York, newdata = newdat_York, se.fit = TRUE, interval = "none", level = 0.95)$fit
D47stats$D47res_lowT_York <- D47stats$D47_mean - predict(D47m_lowT_York, newdata = newdat_York, se.fit = TRUE, interval = "none", level = 0.95)$fit
D47stats$D47res_poly <- D47stats$D47_mean - predict(D47m_poly_MC, newdata = newdat_York, se.fit = TRUE, interval = "none", level = 0.95)$fit
D47stats$D47res_Anderson <- D47stats$D47_mean - (0.0391 * 10 ^ 6 / (D47stats$Temp + 273.15) ^ 2 + 0.154)
D47stats$D47res_Meinicke <- D47stats$D47_mean - (0.0397 * 10 ^ 6 / (D47stats$Temp + 273.15) ^ 2 + 0.1518)
D47stats$D47res_Guo_cc <- D47stats$D47_mean - (((-3.33040 * 10 ^ 9 / (D47stats$Temp + 273.15) ^ 4 + 2.32415 * 10 ^ 7 / (D47stats$Temp + 273.15) ^ 3 - 2.91282 * 10 ^ 3 / (D47stats$Temp + 273.15) ^ 2 - 5.54042 / (D47stats$Temp + 273.15) + 0.23252 + 0.268 - 0.232) - ETH1_GuoCDES) * (ETH1_ICDES-ETH3_ICDES)/(ETH1_GuoCDES-ETH3_GuoCDES) + ETH1_ICDES) * 1.035
D47stats$D47res_Guo_ar <- D47stats$D47_mean - (((-3.43068 * 10 ^ 9 / (D47stats$Temp + 273.15) ^ 4 + 2.35766 * 10 ^ 7 / (D47stats$Temp + 273.15) ^ 3 - 8.06003 * 10 ^ 2 / (D47stats$Temp + 273.15) ^ 2 - 6.90300 / (D47stats$Temp + 273.15) + 0.22893 + 0.268 - 0.232) - ETH1_GuoCDES) * (ETH1_ICDES-ETH3_ICDES)/(ETH1_GuoCDES-ETH3_GuoCDES) + ETH1_ICDES) * 1.035
newdat_York <- data.frame(x = 10 ^6 / (Aisstats$Temp + 273.15) ^ 2)
Aisstats$D47res_York <- Aisstats$D47_mean - predict(D47m_York, newdata = newdat_York, se.fit = TRUE, interval = "none", level = 0.95)$fit
Aisstats$D47res_lowT_York <- Aisstats$D47_mean - predict(D47m_lowT_York, newdata = newdat_York, se.fit = TRUE, interval = "none", level = 0.95)$fit
Aisstats$D47res_poly <- Aisstats$D47_mean - predict(D47m_poly_MC, newdata = newdat_York, se.fit = TRUE, interval = "none", level = 0.95)$fit
Aisstats$D47res_Anderson <- Aisstats$D47_mean - (0.0391 * 10 ^ 6 / (Aisstats$Temp + 273.15) ^ 2 + 0.154)
Aisstats$D47res_Meinicke <- Aisstats$D47_mean - (0.0397 * 10 ^ 6 / (Aisstats$Temp + 273.15) ^ 2 + 0.1518)
Aisstats$D47res_Guo_cc <- Aisstats$D47_mean - (((-3.33040 * 10 ^ 9 / (Aisstats$Temp + 273.15) ^ 4 + 2.32415 * 10 ^ 7 / (Aisstats$Temp + 273.15) ^ 3 - 2.91282 * 10 ^ 3 / (Aisstats$Temp + 273.15) ^ 2 - 5.54042 / (Aisstats$Temp + 273.15) + 0.23252 + 0.268 - 0.232) - ETH1_GuoCDES) * (ETH1_ICDES-ETH3_ICDES)/(ETH1_GuoCDES-ETH3_GuoCDES) + ETH1_ICDES) * 1.035
Aisstats$D47res_Guo_ar <- Aisstats$D47_mean - (((-3.43068 * 10 ^ 9 / (Aisstats$Temp + 273.15) ^ 4 + 2.35766 * 10 ^ 7 / (Aisstats$Temp + 273.15) ^ 3 - 8.06003 * 10 ^ 2 / (Aisstats$Temp + 273.15) ^ 2 - 6.90300 / (Aisstats$Temp + 273.15) + 0.22893 + 0.268 - 0.232) - ETH1_GuoCDES) * (ETH1_ICDES-ETH3_ICDES)/(ETH1_GuoCDES-ETH3_GuoCDES) + ETH1_ICDES) * 1.035
newdat_York <- data.frame(x = 10 ^6 / (violin_data$Temp + 273.15) ^ 2)
violin_data$D47res_York <- violin_data$D47 - predict(D47m_York, newdata = newdat_York, se.fit = TRUE, interval = "none", level = 0.95)$fit
violin_data$D47res_lowT_York <- violin_data$D47 - predict(D47m_lowT_York, newdata = newdat_York, se.fit = TRUE, interval = "none", level = 0.95)$fit
violin_data$D47res_poly <- violin_data$D47 - predict(D47m_poly_MC, newdata = newdat_York, se.fit = TRUE, interval = "none", level = 0.95)$fit
violin_data$D47res_Anderson <- violin_data$D47 - (0.0391 * 10 ^ 6 / (violin_data$Temp + 273.15) ^ 2 + 0.154)
violin_data$D47res_Meinicke <- violin_data$D47 - (0.0397 * 10 ^ 6 / (violin_data$Temp + 273.15) ^ 2 + 0.1518)
violin_data$D47res_Guo_cc <- violin_data$D47 - (((-3.33040 * 10 ^ 9 / (violin_data$Temp + 273.15) ^ 4 + 2.32415 * 10 ^ 7 / (violin_data$Temp + 273.15) ^ 3 - 2.91282 * 10 ^ 3 / (violin_data$Temp + 273.15) ^ 2 - 5.54042 / (violin_data$Temp + 273.15) + 0.23252 + 0.268 - 0.232) - ETH1_GuoCDES) * (ETH1_ICDES-ETH3_ICDES)/(ETH1_GuoCDES-ETH3_GuoCDES) + ETH1_ICDES) * 1.035
violin_data$D47res_Guo_ar <- violin_data$D47 - (((-3.43068 * 10 ^ 9 / (violin_data$Temp + 273.15) ^ 4 + 2.35766 * 10 ^ 7 / (violin_data$Temp + 273.15) ^ 3 - 8.06003 * 10 ^ 2 / (violin_data$Temp + 273.15) ^ 2 - 6.90300 / (violin_data$Temp + 273.15) + 0.22893 + 0.268 - 0.232) - ETH1_GuoCDES) * (ETH1_ICDES-ETH3_ICDES)/(ETH1_GuoCDES-ETH3_GuoCDES) + ETH1_ICDES) * 1.035
# Calculate values for calibrations relative to regressions through our dataset
D47m_York_result_res <- D47m_York_result - D47m_York_result$fit
D47m_York_result_res$x <- D47m_York_result_res$x + D47m_York_result$fit
D47m_York_result_res$Anderson <- Anderson21$D47 - D47m_York_result$fit
D47m_York_result_res$Meinicke <- MeinickeICDES$D47 - D47m_York_result$fit
D47m_York_result_res$Guo_cc <- Guo09$D47_cc - D47m_York_result$fit
D47m_York_result_res$Guo_ar <- Guo09$D47_ar - D47m_York_result$fit
D47m_lowT_York_result_res <- D47m_lowT_York_result - D47m_lowT_York_result$fit
D47m_lowT_York_result_res$x <- D47m_lowT_York_result_res$x + D47m_lowT_York_result$fit
D47m_lowT_York_result_res$Anderson <- Anderson21$D47[1:1001] - D47m_lowT_York_result$fit
D47m_lowT_York_result_res$Meinicke <- MeinickeICDES$D47[1:1001] - D47m_lowT_York_result$fit
D47m_lowT_York_result_res$Guo_cc <- Guo09$D47_cc[1:1001] - D47m_lowT_York_result$fit
D47m_lowT_York_result_res$Guo_ar <- Guo09$D47_ar[1:1001] - D47m_lowT_York_result$fit
D47m_poly_MC_result_res <- D47m_poly_MC_result - D47m_poly_MC_result$fit
D47m_poly_MC_result_res$x <- D47m_poly_MC_result_res$x + D47m_poly_MC_result$fit
D47m_poly_MC_result_res$Anderson <- Anderson21$D47 - D47m_poly_MC_result$fit
D47m_poly_MC_result_res$Meinicke <- MeinickeICDES$D47 - D47m_poly_MC_result$fit
D47m_poly_MC_result_res$Guo_cc <- Guo09$D47_cc - D47m_poly_MC_result$fit
D47m_poly_MC_result_res$Guo_ar <- Guo09$D47_ar - D47m_poly_MC_result$fit
# Prepare summary of calibration offsets
calibration_offset <- data.frame(D47_offset = c(dat$D47res_Anderson[which(dat$type == "bivalve" & (dat$Analysis == "this study" | dat$Analysis == "Bernasconi18"))],
dat$D47res_Meinicke[which(dat$type == "bivalve" & (dat$Analysis == "this study" | dat$Analysis == "Bernasconi18"))],
dat$D47res_Guo_cc[which(dat$type == "bivalve" & (dat$Analysis == "this study" | dat$Analysis == "Bernasconi18"))],
dat$D47res_Guo_ar[which(dat$type == "bivalve" & (dat$Analysis == "this study" | dat$Analysis == "Bernasconi18"))]),
D47_SD = rep(dat$D47_SD[which(dat$type == "bivalve" & (dat$Analysis == "this study" | dat$Analysis == "Bernasconi18"))], 4),
Calibration = c(rep("Anderson et al., 2021", length(dat$D47res_Meinicke[which(dat$type == "bivalve" & (dat$Analysis == "this study" | dat$Analysis == "Bernasconi18"))])),
rep("Meinicke et al., 2020", length(dat$D47res_Meinicke[which(dat$type == "bivalve" & (dat$Analysis == "this study" | dat$Analysis == "Bernasconi18"))])),
rep("Guo et al., 2009 (calcite)", length(dat$D47res_Meinicke[which(dat$type == "bivalve" & (dat$Analysis == "this study" | dat$Analysis == "Bernasconi18"))])),
rep("Guo et al., 2009 (aragonite)", length(dat$D47res_Meinicke[which(dat$type == "bivalve" & (dat$Analysis == "this study" | dat$Analysis == "Bernasconi18"))])))
)
# Calculate the temperature equivalent of the calibration offset
mean_Temp <- mean(dat$Temp[which(dat$type == "bivalve" & (dat$Analysis == "this study" | dat$Analysis == "Bernasconi18"))])
mean_D47 <- 0.0391 * 10 ^ 6 / (mean_Temp + 273.13) ^ 2 + 0.154
# Statistics of difference with calibration
Calibration_offset_stats <- calibration_offset %>% # Summarize D47 offset statistics
group_by(Calibration) %>%
summarize(
N = n(), # Calculate the number of modelled values, excluding NA's
D47_offset_Average = binmeans(x = D47_offset, x_sd = D47_SD, output = "mean"),
D47_offset_SD = binmeans(x = D47_offset, x_sd = D47_SD, output = "SD"),
D47_offset_SE = D47_offset_SD / sqrt(N - 1), # Calculate the standard error
D47_offset_CL95 = qt(0.95, N - 1) * D47_offset_SE # Calculate the 95% confidence level
) %>%
ungroup()
Calibration_offset_stats$Temp_offset_Average <- sqrt(0.0391 * 10 ^ 6 / (mean_D47 - Calibration_offset_stats$D47_offset_Average - 0.154)) - 273.15 - mean_Temp
Calibration_offset_stats$Temp_offset_SD <- sqrt(0.0391 * 10 ^ 6 / (mean_D47 - Calibration_offset_stats$D47_offset_Average - Calibration_offset_stats$D47_offset_SD - 0.154)) - 273.15 - mean_Temp - Calibration_offset_stats$Temp_offset_Average
Calibration_offset_stats$Temp_offset_SE <- sqrt(0.0391 * 10 ^ 6 / (mean_D47 - Calibration_offset_stats$D47_offset_Average - Calibration_offset_stats$D47_offset_SE - 0.154)) - 273.15 - mean_Temp - Calibration_offset_stats$Temp_offset_Average
Calibration_offset_stats$Temp_offset_CL95 <- sqrt(0.0391 * 10 ^ 6 / (mean_D47 - Calibration_offset_stats$D47_offset_Average - Calibration_offset_stats$D47_offset_CL95 - 0.154)) - 273.15 - mean_Temp - Calibration_offset_stats$Temp_offset_Average
View(calibration_offset)
View(dat)
mean(dat$D47res_Anderson)
sd(dat$D47res_Anderson)
sd(dat$D47res_Anderson)/sqrt(length(dat$D47res_Anderson) - 1)
qt(0.05, length(dat$D47res_Anderson))
qt(0.95, length(dat$D47res_Anderson))
qt(0.95, length(dat$D47res_Anderson)) * sd(dat$D47res_Anderson)/sqrt(length(dat$D47res_Anderson) - 1)
mean(dat$D47res_Anderson[-which(dat$Analysis == "Muller17")])
qt(0.95, length(dat$D47res_Anderson[-which(dat$Analysis == "Muller17")])) * sd(dat$D47res_Anderson[-which(dat$Analysis == "Muller17")])/sqrt(length(dat$D47res_Anderson[-which(dat$Analysis == "Muller17")]) - 1)
View(Calibration_offset_stats)
load("C:/Users/niels/Dropbox/Research/postdoc/UNBIAS/growth experiments/LAICPMS_Sr_spiking/Batch2/LA data/LA_combined_batch2.Rdata")
View(LA_combined)
Profile_plot_Sr_offset_all <- ggplot(LA_combined) +
geom_point(data = subset(LA_combined, !(Specimen_id %in% c(5, 7, 9))),
aes(Depth,
SrCa + as.numeric(Specimen_id) - 1,
col = Species),
alpha = 0.1,
size = 0.1) +
scale_y_continuous("[Sr]/[Ca] (mmol/mol)",
breaks = seq(0, 20, 1),
labels = seq(0, 20, 1),
limits = c(0, 20)) +
scale_x_continuous("Distance from ventral margin [mm]") +
ggtitle("Offset (+ 1) Sr/Ca curves") +
theme_bw()
require(tidyverse)
require(gridExtra)
Profile_plot_Sr_offset_all <- ggplot(LA_combined) +
geom_point(data = subset(LA_combined, !(Specimen_id %in% c(5, 7, 9))),
aes(Depth,
SrCa + as.numeric(Specimen_id) - 1,
col = Species),
alpha = 0.1,
size = 0.1) +
scale_y_continuous("[Sr]/[Ca] (mmol/mol)",
breaks = seq(0, 20, 1),
labels = seq(0, 20, 1),
limits = c(0, 20)) +
scale_x_continuous("Distance from ventral margin [mm]") +
ggtitle("Offset (+ 1) Sr/Ca curves") +
theme_bw()
x11(); plot(Profile_plot_Sr_offset_all)
Profile_plot_Sr_offset_all <- ggplot(LA_combined) +
geom_point(data = subset(LA_combined, !(Specimen_id %in% c(5, 7, 9))),
aes(Depth,
SrCa + as.numeric(Specimen_id) - 1,
col = Species),
alpha = 0.1,
size = 0.1) +
geom_smooth() +
scale_y_continuous("[Sr]/[Ca] (mmol/mol)",
breaks = seq(0, 20, 1),
labels = seq(0, 20, 1),
limits = c(0, 20)) +
scale_x_continuous("Distance from ventral margin [mm]") +
ggtitle("Offset (+ 1) Sr/Ca curves") +
theme_bw()
plot(Profile_plot_Sr_offset_all)
Profile_plot_Sr_offset_all <- ggplot(LA_combined) +
geom_point(data = subset(LA_combined, !(Specimen_id %in% c(5, 7, 9))),
aes(Depth,
SrCa + as.numeric(Specimen_id) - 1,
col = Species),
alpha = 0.1,
size = 0.1) +
geom_smooth(aes(x = Depth,
y = SrCa + as.numeric(Specimen_id) - 1,
col = Species)) +
scale_y_continuous("[Sr]/[Ca] (mmol/mol)",
breaks = seq(0, 20, 1),
labels = seq(0, 20, 1),
limits = c(0, 20)) +
scale_x_continuous("Distance from ventral margin [mm]") +
ggtitle("Offset (+ 1) Sr/Ca curves") +
theme_bw()
plot(Profile_plot_Sr_offset_all)
?geom_smooth
plot(LA_combined$Distance)
plot(LA_combined$Xpos, LA_combined$Ypos)
x11(); plot(LA_combined$Xpos, LA_combined$Ypos)
read.csv("inst/extdata/Virtual_shell.csv")
dat <- read.csv("inst/extdata/Virtual_shell.csv")
?duplicated
duplicated(dat$D)
all(duplicated(dat$D))
all(c("TRUE", "TRUE", "FALSE"))
all(c("TRUE", "TRUE", "TRUE"))
TRUE in duplicated(dat$D)
(TRUE in duplicated(dat$D))
(TRUE %in% duplicated(dat$D))
(TRUE %in% c("TRUE", "TRUE", "FALSE"))
which("TRUE", "TRUE", "FALSE")
which(c("TRUE", "TRUE", "FALSE"))
which(c("TRUE", "TRUE", "FALSE") == TRUE)
which(duplicated(dat$D) == TRUE)
length(duplicated(dat$D))
length(dat$D)
test <- data.frame(D = c(0, 1, 2, 3, 3, 4, 5, 6, 6, 10), d18O = 1:10)
duplicated(test$D)
dat <- dat[-which(duplicated(dat$D) == TRUE), ]
dat <- read.csv("inst/extdata/Virtual_shell.csv")
?data,import
?data_import
require(ShellChron)
?data_import
data_import("inst/extdata/Virtual_shell.csv")
import_list <- data_import("inst/extdata/Virtual_shell.csv")
?getwd()
getwd()
import_list <- data_import("E:/Dropbox/Research/Manuscripts/[2022Published] GMD - Bivalve age model/tests/Virtual_shell_duplicates.csv")
import_list <- data_import("inst/extdata/Virtual_shell.csv")
file_name <- "E:/Dropbox/Research/Manuscripts/[2022Published] GMD - Bivalve age model/tests/Virtual_shell_duplicates.csv"
dat <- read.csv(file_name, header = T)
dat <- dat[order(dat[, 1]),] # Order data by D
# Check the structure of the import dataframe
if(ncol(dat) == 5){ # If the number of columns checks out
# Check the column names, and rename them if necessary
if(!all(colnames(dat) == c("D", "d18Oc", "YEARMARKER", "D_err", "d18Oc_err"))){
colnames(dat) <- c("D", "d18Oc", "YEARMARKER", "D_err", "d18Oc_err")
}
}else if(ncol(dat) == 3){ # If SD columns are omitted
# Check the names of provided columns, and rename them if necessary
if(!all(colnames(dat) == c("D", "d18Oc", "YEARMARKER"))){
colnames(dat) <- c("D", "d18Oc", "YEARMARKER")
}
dat$D_err <- rep(0, nrow(dat))
dat$d18Oc_err <- rep(0, nrow(dat))
}else{
return("ERROR: Input data does not match the default input data format")
}
# Check for duplicate depth values
if(TRUE %in% duplicated(dat$D)){
dat <- dat[-which(duplicated(dat$D) == TRUE), ] # Remove duplicated depth values
print("WARNING: Duplicated depth values were found and removed")
}
# Define sliding window based on indicated year markers
YEARMARKER <- which(dat$YEARMARKER == 1) # Read position of yearmarkers in data.
yearwindow <- diff(which(dat$YEARMARKER == 1)) # Calculate the number of datapoints in each year between consecutive year markers
if(length(yearwindow) > 1){
dynwindow <- approx( # Interpolate between the numbers of annual datapoints to create list of starting positions of growth windows and their size for age modeling
x = YEARMARKER[-length(YEARMARKER)],
y = yearwindow,
xout = 1:(length(dat$D) - yearwindow[length(yearwindow)] + 1), # X indicates starting positions of windows used for age modeling
method = "linear",
rule = 2 # Window sizes for beginning given as NA's, for end equal to last value
)
dynwindow$y <- round(dynwindow$y) # Round off window sizes to integers
dynwindow$y[dynwindow$y < 10] <- 10 # Eliminate small window sizes to lend confidence to the sinusoidal fit
overshoot<-which(dynwindow$x + dynwindow$y > length(dat[,1])) # Find windows that overshoot the length of dat
dynwindow$x <- dynwindow$x[-overshoot] # Remove overshooting windows
dynwindow$y <- dynwindow$y[-overshoot] # Remove overshooting windows
if((length(dynwindow$x) + dynwindow$y[length(dynwindow$x)] - 1) < length(dat[, 1])){ # Increase length of the final window in case samples at the end are missed due to jumps in window size
dynwindow$y[length(dynwindow$y)] <- dynwindow$y[length(dynwindow$y)] + (length(dat[, 1]) - (length(dynwindow$x) + dynwindow$y[length(dynwindow$x)] - 1))
}
}else if(length(yearwindow) == 1){ # Catch exception of datasets with only two yearmarkers
dynwindow <- data.frame(
x = 1:(length(dat$D) - yearwindow[length(yearwindow)] + 1),
y = rep(yearwindow, (length(dat$D) - yearwindow[length(yearwindow)] + 1))
)
}else{
return("ERROR: Need at least 2 year markers to estimate window size")
}
devtools::install_github("nielsjdewinter/ShellChron")
require(ShellChron)
?wrap_function
getwd()
wrap_function(path = "E:/Dropbox/Research/Manuscripts/[2022Published] GMD - Bivalve age model/tests",
file_name = "Virtual_shell_duplicates.csv",
transfer_function = "KimONeil97",
t_int = 1,
T_per = 365,
d18Ow = 0,
t_maxtemp = 183,
sinfit = TRUE,
MC = 1000,
plot = TRUE,
plot_export = TRUE,
export_raw = TRUE,
export_path = "E:/Dropbox/Research/Manuscripts/[2022Published] GMD - Bivalve age model/tests/test_results"
)
devtools::document()
devtools::install()
devtools::document()
devtools::install()
require(ShellChron
)
require(ShellChron)
dat <- as.data.frame(seq(1000, 40000, 1000))
colnames(dat) <- "D"
dat$d18Oc <- sin((2 * pi * (seq(1, 40, 1) - 8 + 7 / 4)) / 7)
dat$YEARMARKER <- c(0, rep(c(0, 0, 0, 0, 0, 0, 1), 5), 0, 0, 0, 0)
dat$D_err <- rep(100, 40)
dat$d18Oc_err <- rep(0.1, 40)
importlist <- data_import_object(object_name = dat)
