# stdevs per sample
N = dplyr::n_distinct(d18O_mod, na.rm = TRUE), # Calculate the
# number of modelled values, excluding NA's
se.d18O_mod = sd.d18O_mod / sqrt(N), # Calculate the standard error
CL95.d18O_mod = qt(0.95, N) * se.d18O_mod # Calculate the 95%
# confidence level
)
d18Ostats$sd.d18O_mod[which(d18Ostats$N == 1)] <- NaN
GRtidy <- tidyr::gather(as.data.frame(resultarray[, , 4]), "window", "GR",
(length(dat[1, ]) + 1):length(resultarray[1, , 1]), factor_key = TRUE)
# Convert modelled growth rate results to Tidy data for plotting
GRtidy$weights <- weightstidy$weight # Add weights to GRtidy
GRstats <- GRtidy %>% # Summarize modelled growth rate statistics
ggpubr::group_by(D) %>%
dplyr::summarize(
mean.GR = mean(GR[GR>0.1], na.rm = TRUE),  # Calculate means per
# sample, excluding NA's and instances where growth rate is
# near-zero
sd.GR = sd.wt(GR[GR>0.1], weights, na.rm = TRUE),  # Calculate
# stdevs per sample, excluding NA's and instances where growth
# rate is near-zero
N = dplyr::n_distinct(GR[GR>0.1], na.rm = TRUE), # Calculate the
# number of modelled values, excluding NA's and instances where
# growth rate is near-zero
se.GR = sd.GR / sqrt(N), # Calculate the standard error
CL95.GR = qt(0.95, N) * se.GR # Calculate the 95% confidence level
)
GRstats$sd.GR[which(GRstats$N == 1)] <- NaN
Ttidy <- tidyr::gather(as.data.frame(resultarray[, , 5]), "window", "SST",
(length(dat[1, ]) + 1):length(resultarray[1, , 1]), factor_key = TRUE)
# Convert modelled temperature results to Tidy data for plotting
Ttidy$weights <- weightstidy$weight # Add weights to Ttidy
Tstats <- Ttidy %>% # Summarize modelled growth rate statistics
ggpubr::group_by(D) %>%
dplyr::summarize(
mean.SST = weighted.mean(SST[SST>0.1], na.rm = TRUE),  # Calculate
# means per sample, excluding NA's and instances where SSTowth
# rate is near-zero
sd.SST = sd.wt(SST[SST>0.1], weights, na.rm = TRUE),  # Calculate
# stdevs per sample, excluding NA's and instances where SSTowth rate
# is near-zero
N = dplyr::n_distinct(SST[SST>0.1], na.rm = TRUE), # Calculate the
# number of modelled values, excluding NA's and instances where
# SSTowth rate is near-zero
se.SST = sd.SST / sqrt(N), # Calculate the standard error
CL95.SST = qt(0.95, N) * se.SST # Calculate the 95% confidence level
)
Tstats$sd.SST[which(Tstats$N == 1)] <- NaN
parmat2 <- data.frame(rownames(parmat), parmat)
colnames(parmat2)[1] <- "parameter"
partidy <- tidyr::gather(parmat2, "window", "par_value",
2:length(parmat2[1,]), factor_key = TRUE)
parstats <- partidy %>% # Summarize model parameters
ggpubr::group_by(parameter) %>%
dplyr::summarize(
means = mean(par_value), # Calculate means per parameter
stdev = sd(par_value), # Calculate standard deviation per parameter
N = dplyr::n(), # Count number of modelled values per parameter
# (= equal to number of windows)
se.pars = stdev / sqrt(N), # Calculate standard error
CL95 = qt(0.95, N) * se.pars
)
if(MC > 0){
print("Recalculating export statistics by including
propagated uncertainties")
# Include errors propagated from those on D and d18Oc data into the
# statistics
# Propagate errors on modelled d18O
d18Otidy_err <- d18Otidy
d18Otidy_err$SD <- tidyr::gather(as.data.frame(resultarray[, , 6]),
"window", "d18O", (length(dat[1, ]) + 1):length(
resultarray[1, , 1]), factor_key = TRUE)$d18O
# Convert modelled time errors to Tidy data for plotting
d18Otidy_err$N <- dynwindow$y[as.numeric(d18Otidy$window)] # Add window
# size for calculating pooled SD
d18Otidy_err <- d18Otidy_err[-which(is.na(d18Otidy_err$d18O_mod)), ]
# Remove empty cells in matrix
d18Otidy_err$SD[which(d18Otidy_err$SD == 0)] <-
min(d18Otidy_err$SD[which(d18Otidy_err$SD > 0)])
# Replace zeroes with smallest SD to prevent division by zero
d18Ostats2 <- d18Otidy_err %>% # Summarize modelled d18O statistics
ggpubr::group_by(D) %>%
dplyr::summarize(
weighted.mean.d18O = weighted.mean(d18O_mod, 1 / SD ^ 2 * weights,
na.rm = TRUE),  # Calculate weighted means per sample
pooled.sd.d18O = sqrt(sum(SD ^ 2 * (N - 1) * weights,
na.rm = TRUE) / ((sum(N, na.rm = TRUE) - dplyr::n()) *
mean(weights))) # Calculate pooled standard deviation resulting
# from error propagations and the weighted mean of the variances
# taking weights derived from position in the window into
# account
)
# Aggregate propagated errors into statistics matrices
d18Ostats$mean.d18O_mod <- d18Ostats2$weighted.mean.d18O # Replace means
# by weighed means, taking into account the propagated error on
# individual estimates
d18Ostats$sd.d18O_mod <- sqrt(d18Ostats$sd.d18O_mod ^ 2 +
d18Ostats2$pooled.sd.d18O ^2) # Combine errors from the model and
# the errors on input
d18Ostats$se.d18O_mod <- d18Ostats$sd.d18O_mod / sqrt(d18Ostats$N)
# Propagate new errors onto standard error
d18Ostats$CL95.d18O_mod <- qt(0.95, d18Ostats$N) * d18Ostats$se.d18O_mod
# Propagate new errors onto confidence interval
# Propagate errors on Time of Day calculations
JDtidy_err <- JDtidy
JDtidy_err$SD <- tidyr::gather(as.data.frame(resultarray[, , 7]),
"window", "Day", (length(dat[1, ]) + 1):length(resultarray[1, , 1]),
factor_key = TRUE)$Day # Convert modelled time errors to Tidy data
# for plotting
JDtidy_err$N <- dynwindow$y[as.numeric(JDtidy$window)] # Add window size
# for calculating pooled SD
JDtidy_err <- JDtidy_err[-which(is.na(JDtidy_err$Day)), ] # Remove empty
# cells in matrix
JDtidy_err$SD[which(JDtidy_err$SD == 0)] <-
min(JDtidy_err$SD[which(JDtidy_err$SD > 0)]) # Replace zeroes with
# smallest SD to prevent division by zero
JDstats2 <- JDtidy_err %>% # Summarize modelled JD statistics
ggpubr::group_by(D) %>%
dplyr::summarize(
weighted.mean.day = weighted.mean(Day, 1 / SD ^ 2 * weights,
na.rm = TRUE),  # Calculate weighted means per sample
pooled.sd.day = sqrt(sum(SD ^ 2 * (N - 1) * weights, na.rm = TRUE) /
((sum(N, na.rm = TRUE) - dplyr::n()) * mean(weights)))
# Calculate pooled standard deviation resulting from error
# propagations and the weighted mean of the variances taking
# weights derived from position in the window into account
)
# Aggregate propagated errors into statistics matrices
JDstats$mean.day <- JDstats2$weighted.mean.day # Replace means by
# weighed means, taking into account the propagated error on individual
# estimates
JDstats$sd.day <- sqrt(JDstats$sd.day ^ 2 + JDstats2$pooled.sd.day ^2)
# Combine errors from the model and the errors on input
JDstats$se.day <- JDstats$sd.day / sqrt(JDstats$N) # Propagate new
# errors onto standard error
JDstats$CL95.day <- qt(0.95, JDstats$N) * JDstats$se.day # Propagate new
# errors onto confidence interval
# Propagate errors on modelled growth rate
GRtidy_err <- GRtidy
GRtidy_err$SD <- tidyr::gather(as.data.frame(resultarray[, , 8]),
"window", "GR", (length(dat[1, ]) + 1):length(resultarray[1, , 1]),
factor_key = TRUE)$GR # Convert modelled time errors to Tidy
# data for plotting
GRtidy_err$N <- dynwindow$y[as.numeric(GRtidy$window)] # Add window size
# for calculating pooled SD
GRtidy_err <- GRtidy_err[-which(is.na(GRtidy_err$GR)), ] # Remove empty
# cells in matrix
GRtidy_err$SD[which(GRtidy_err$SD == 0)] <-
min(GRtidy_err$SD[which(GRtidy_err$SD > 0)]) # Replace zeroes with
# smallest SD to prevent division by zero
GRstats2 <- GRtidy_err %>% # Summarize modelled GR statistics
ggpubr::group_by(D) %>%
dplyr::summarize(
weighted.mean.GR = weighted.mean(GR, 1 / SD ^ 2 * weights,
na.rm = TRUE),  # Calculate weighted means per sample
pooled.sd.GR = sqrt(sum(SD ^ 2 * (N - 1) * weights, na.rm = TRUE) /
((sum(N, na.rm = TRUE) - dplyr::n()) * mean(weights)))
# Calculate pooled standard deviation resulting from error
# propagations and the weighted mean of the variances taking
# weights derived from position in the window into account
)
# Aggregate propagated errors into statistics matrices
GRstats$mean.GR <- GRstats2$weighted.mean.GR # Replace means by weighed
# means, taking into account the propagated error on individual estimates
GRstats$sd.GR <- sqrt(GRstats$sd.GR ^ 2 + GRstats2$pooled.sd.GR ^2)
# Combine errors from the model and the errors on input
GRstats$se.GR <- GRstats$sd.GR / sqrt(GRstats$N) # Propagate new errors
# onto standard error
GRstats$CL95.GR <- qt(0.95, GRstats$N) * GRstats$se.GR # Propagate new
# errors onto confidence interval
# Propagate errors on modelled temperature
Ttidy_err <- Ttidy
Ttidy_err$SD <- tidyr::gather(as.data.frame(resultarray[, , 9]),
"window", "T", (length(dat[1, ]) + 1):length(resultarray[1, , 1]),
factor_key = TRUE)$T # Convert modelled time errors to Tidy data
#for plotting
Ttidy_err$N <- dynwindow$y[as.numeric(Ttidy$window)] # Add window size
# for calculating pooled SD
Ttidy_err <- Ttidy_err[-which(is.na(Ttidy_err$SST)), ] # Remove empty
# cells in matrix
Ttidy_err$SD[which(Ttidy_err$SD == 0)] <-
min(Ttidy_err$SD[which(Ttidy_err$SD > 0)]) # Replace zeroes with
# smallest SD to prevent division by zero
Tstats2 <- Ttidy_err %>% # Summarize modelled T statistics
ggpubr::group_by(D) %>%
dplyr::summarize(
weighted.mean.SST = weighted.mean(SST, 1 / SD ^ 2 * weights,
na.rm = TRUE),  # Calculate weighted means per sample
pooled.sd.SST = sqrt(sum(SD ^ 2 * (N - 1) * weights, na.rm = TRUE) /
((sum(N, na.rm = TRUE) - dplyr::n()) * mean(weights))) # Calculate
# pooled standard deviation resulting from error propagations and
# the weighted mean of the variances taking weights derived from
# position in the window into account
)
# AgTegate propagated errors into statistics matrices
Tstats$mean.SST <- Tstats2$weighted.mean.SST # Replace means by weighed
# means, taking into account the propagated error on individual
# estimates
Tstats$sd.SST <- sqrt(Tstats$sd.SST ^ 2 + Tstats2$pooled.sd.SST ^2)
# Combine errors from the model and the errors on input
Tstats$se.SST <- Tstats$sd.SST / sqrt(Tstats$N) # Propagate new errors
# onto standard error
Tstats$CL95.SST <- qt(0.95, Tstats$N) * Tstats$se.SST # Propagate new
# errors onto confidence interval
print("Preparing plots")
}
if(plot == TRUE | plot_export == TRUE){ # Check if plots are needed
# Create depth-time plot
Dtplot <- ggplot2::ggplot(JDtidy, ggplot2::aes(D, Day)) +
ggplot2::geom_point(ggplot2::aes(colour = d18Oc)) +
ggplot2::scale_colour_gradient2(midpoint = mean(JDtidy$d18Oc)) +
ggplot2::geom_line(data = JDstats, ggplot2::aes(D, mean.day),
size = 1) +
ggplot2::geom_line(data = JDstats, ggplot2::aes(D, mean.day +
CL95.day), size = 1, alpha = 0.5) +
ggplot2::geom_line(data = JDstats, ggplot2::aes(D, mean.day -
CL95.day), size = 1, alpha = 0.5) +
ggplot2::ggtitle("Plot of Height vs. Time") +
ggplot2::xlab("Record length") +
ggplot2::scale_y_continuous("Age (days)", seq(0, 365 *
ceiling(max(JDstats$mean.day + JDstats$CL95.day,
na.rm = TRUE) / 365), 365))
# Create d18O plot
d18Oplot <- ggplot2::ggplot(d18Otidy, ggplot2::aes(D, d18Oc)) +
ggplot2::geom_point() +
ggplot2::geom_line(data = d18Ostats, ggplot2::aes(D, mean.d18O_mod),
size = 1) +
ggplot2::geom_line(data = d18Ostats, ggplot2::aes(D, mean.d18O_mod +
CL95.d18O_mod, alpha = 0.5, col = "darkblue"), size = 1) +
ggplot2::geom_line(data = d18Ostats, ggplot2::aes(D, mean.d18O_mod -
CL95.d18O_mod, alpha = 0.5, col = "darkred"), size = 1) +
ggplot2::ggtitle("Plot of measured and modelled d18O vs.
Record Length") +
ggplot2::xlab("Record length") +
ggplot2::ylab("d18O_carbonate") +
ggplot2::theme(legend.position = "none") # Remove legend
# Create growth rate plot
GRplot <- ggplot2::ggplot(GRtidy, ggplot2::aes(D, GR)) +
ggplot2::geom_point(ggplot2::aes(colour = d18Oc)) +
ggplot2::scale_colour_gradient2(midpoint = mean(JDtidy$d18Oc)) +
ggplot2::geom_line(data = GRstats, ggplot2::aes(D, mean.GR),
size = 1) +
ggplot2::geom_line(data = GRstats, ggplot2::aes(D, mean.GR +
CL95.GR, alpha = 0.5), size = 1) +
ggplot2::geom_line(data = GRstats, ggplot2::aes(D, mean.GR -
CL95.GR, alpha = 0.5), size = 1) +
ggplot2::ggtitle("Plot of modelled growth rate vs Record Length") +
ggplot2::xlab("Record length") +
ggplot2::ylab("Growth rate") +
ggplot2::theme(legend.position = "none") # Remove legend
Combined_plots <- ggpubr::ggarrange(Dtplot, d18Oplot, GRplot,
labels = c("A", "B", "C"), ncol = 3, nrow = 1) # Combine plots
if(plot == TRUE){
dev.new()
print(Combined_plots)
}
if(plot_export == TRUE){
pdf("Model result plots.pdf", width = 30, height = 10)
print(Combined_plots)
dev.off()
}
}
print("Start exporting files to directory")
if(export_raw == TRUE){
# Write away all raw results of modelling
write.csv(resultarray[, , 1], "modelled_d18O_raw.csv")
write.csv(resultarray[, , 2], "residuals_raw.csv")
write.csv(resultarray[, , 3], "Day_of_year_raw.csv")
write.csv(resultarray[, , 4], "Instantaneous_growth_rate_raw.csv")
write.csv(resultarray[, , 5], "SST_raw.csv")
write.csv(resultarray[, , 6], "Modelled_d18O_SD_raw.csv")
write.csv(resultarray[, , 7], "Day_of_Year_SD_raw.csv")
write.csv(resultarray[, , 8], "Instantaneous_growth_rate_SD_raw.csv")
write.csv(resultarray[, , 9], "SST_SD_raw.csv")
write.csv(parmat, "modelled_parameters_raw.csv")
}
# Write avay summary statistics of modelling
write.csv(JDstats, "Age_model_results.csv")
write.csv(d18Ostats, "d18O_model_results.csv")
write.csv(GRstats, "Growth_rate_results.csv")
write.csv(Tstats, "SST_results.csv")
write.csv(parstats, "Model_parameter_results.csv")
print("DONE!")
}
growth_rate_curve <- function(G_par, # Function to create growth rate as
# function of time based on parameters
years = 1, # Default number of years of the record = 1
t_int = 1 # Default time interval = 1 day
){
G_amp <- G_par[1] # Seasonal range in Growth rate (um/d)
G_per <- G_par[2] # Period of growth rate seasonality (days)
G_pha <- G_par[3] # Timing of peak in growth rate (day of the year)
G_av <- G_par[4] # Annual average growth rate (um/d)
G_skw <- G_par[5] # Skewness factor in growth rate sinusoid (-)
t <- seq(0, G_per, t_int) # Define time axis based on the number of days in
# a year and the time interval
GR <- rep(NA, length(t)) # Create empty growth rate vector
# Build GR vector piece by piece based on parameters
# Check if t is between the time of maximum growth and the subsequent time
# of minimum growth but before the peak, add one period's length if it is
if(length(t[(((t - G_pha) %% G_per) < (G_per * (100 - G_skw) / 100)) &
(t < G_pha)]) > 0){ # Catch "replacement has length zero" errors
GR[(((t - G_pha) %% G_per) < (G_per * (100 - G_skw) / 100)) &
(t < G_pha)] <- G_av + G_amp/2 * sin(2 * pi * (t[(((t - G_pha) %%
G_per) < (G_per * (100 - G_skw) / 100)) & (t < G_pha)] +
G_per - G_pha + (G_per * (100 - G_skw) / 50) / 4) /
(G_per * (100 - G_skw) / 50))
}
# Check if t is between the time of maximum growth and the subsequent time
# of minimum growth but still after the peak
if(length(t[(((t - G_pha) %% G_per) < (G_per * (100 - G_skw) / 100)) &
(t >= G_pha)]) > 0){ # Catch "replacement has length zero" errors
GR[(((t - G_pha) %% G_per) < (G_per * (100 - G_skw) / 100)) &
(t >= G_pha)] <- G_av + G_amp/2 * sin(2 * pi * (t[(((t - G_pha) %%
G_per) < (G_per * (100 - G_skw) / 100)) & (t >= G_pha)] -
G_pha + (G_per * (100 - G_skw) / 50) / 4) / (G_per *
(100 - G_skw) / 50))
}
# Check if t is between the time of maximum growth and the previous time of
# minimum growth but still before the peak, subtract one period's length if
# it is
if(length(t[(((t - G_pha) %% G_per) >= (G_per * (100 - G_skw) / 100)) &
(t > G_pha)]) > 0){
GR[(((t - G_pha) %% G_per) >= (G_per * (100 - G_skw) / 100)) &
(t > G_pha)] <- G_av + G_amp/2 * sin(2 * pi * (t[(((t - G_pha) %%
G_per) >= (G_per * (100 - G_skw) / 100)) & (t > G_pha)] -
G_per - G_pha + (G_per * G_skw / 50) / 4) /
(G_per * G_skw / 50))
}
if(length(t[(((t - G_pha) %% G_per) >= (G_per * (100 - G_skw) / 100)) &
(t <= G_pha)]) > 0){
GR[(((t - G_pha) %% G_per) >= (G_per * (100 - G_skw) / 100)) &
(t <= G_pha)] <- G_av + G_amp/2 * sin(2 * pi * (t[(((t - G_pha) %%
G_per) >= (G_per * (100 - G_skw) / 100)) & (t <= G_pha)] -
G_pha + (G_per * G_skw / 50) / 4) / (G_per * G_skw / 50))
}
# Replace negative growth rates with zeroes
GR[GR < 0] <- 0
# Multiply t and GR with number of years
if(years > 1){
t <- c(t, rep(t[-1], years - 1) + rep(seq(365, 365 * (years - 1), 365),
each = 365))
GR <- c(GR, rep(GR[-1], years - 1))
}
# Collate results and export
res <- cbind(t, GR)
return(res)
}
mc_err_orth <- function(x, x_err, y, y_err, X, Y, MC){ # Function to propagate
# combined errors on x and y on the modelled X and Y values by means of
# orthogonal projection of x and y uncertainty on the model curve
xmat <- matrix(rnorm(MC * length(x)), nrow = length(x)) * x_err +
matrix(rep(x, MC), nrow = length(x)) # Create matrix of simulated
# X values
ymat <- matrix(rnorm(MC * length(y)), nrow = length(y)) * y_err +
matrix(rep(y, MC), nrow = length(y)) # Create matrix of simulated
# Y values
Xarray <- sqrt( # create array of the length of vectors connecting each MC
# simulated x-y pair and each modelled X-Y pair.
((outer(xmat, X, FUN = "-") - mean(outer(xmat, X, FUN = "-"))) /
sd(outer(xmat, X, FUN = "-"))) ^ 2 + # Square of the normalized
# difference between MC-simulated X values and modelled D
((outer(ymat, Y[, 2], FUN = "-") - mean(outer(ymat, Y[, 2],
FUN = "-"))) / sd(outer(ymat, Y[, 2], FUN = "-"))) ^ 2
# Square of the normalized difference between MC-simulated y values
# and modelled Y values
)
posmat <- apply(Xarray, c(1,2), which.min) # For each MC-simulated x-y pair,
# find the position of the closest model value
Xsimmat <- matrix(X[posmat], nrow = length(x)) # Find the X value that
# belongs to each position in posmat
X_comb <- apply(Xsimmat, 1, mean) # calculate the mean value in X domain
# resulting from orthogonal projection of errors on X and Y from variability
# within the X values
Ysimmat <- matrix(Y[posmat, 2], nrow = length(y)) # Find the Y value that
# belongs to each position in posmat
Y_comb <- approx( # Interpolate modelled temperature values to positions
# along the record.
x = X,
y = Y[, 2],
xout = X_comb,
method = "linear",
rule = 2
)
# Y_comb <- apply(Ysimmat, 1, mean) # calculate the mean value in Y domain
# resulting from orthogonal projection of errors on X and Y from variability
# within the X values
X_err_comb <- apply(Xsimmat, 1, sd) # calculate the standard deviation in X
# domain resulting from orthogonal projection of errors on X and Y from
# variability within the X values
Y_err_comb <- apply(Ysimmat, 1, sd) # calculate the standard deviation in Y
# domain resulting from orthogonal projection of errors on X and Y from
# variability within the X values
result <- data.frame(
X = X_comb,
X_err = X_err_comb,
Y = Y_comb$y,
Y_err = Y_err_comb
)
return(result)
}
mc_err_form <- function(x, x_err, y, y_err, X, Y, MC){
xmat <- matrix(rnorm(MC * length(x)), nrow = length(x)) * x_err +
matrix(rep(x, MC), nrow = length(x)) # Create matrix of simulated
# X values
Xpos <- apply(abs(outer(xmat, X, FUN = "-")), c(1,2), which.min) # find
# closest position in X vector (day) for each simulated X value
Xpos_stat <- cbind(rowMeans(Xpos), apply(Xpos, 1, sd)) # Find mean and
# standard deviation of positions in X vector (day) for each sample
ymat <- matrix(rnorm(MC * length(y)), nrow = length(y)) * y_err +
matrix(rep(Y[Xpos_stat[, 1], 2], MC), nrow = length(y)) # Create matrix
# of Monte Carlo simulated Y values projected on the X-Y curve
Xpos_mat <- outer(round(Xpos_stat[, 1]), seq(-20, 20, 1), "+") %%
length(D) + 1 # Create matrix of D positions around the mean position
# for each sample to match with simulated Y values.
# Window is +/- 20 positions
Xpos_matO <- matrix(Y[Xpos_mat, 2], nrow = length(x)) # Find Y values for
# each position in Xpos_mat
Xpos_matO2 <- apply(outer(Xpos_matO, ymat, FUN = "-"), c(2,4), diag) # Match
# and subtract each simulated Y value with the local environment of Y values
# in the X-Y curve
Ypos <- apply(abs(Xpos_matO2), c(1, 3), which.min) + Xpos_stat[, 1] # Find
# the position of the closest Y value in the Y window (Xpos_matO) for each
# Y simulation
Ypos_stat <- cbind(rowMeans(Ypos), apply(Ypos, 1, sd)) # Find mean and
# standard deviation of positions in X vector (day) for each sample
Ypos_stat[which(Ypos_stat[, 2] == 0), 2] <- sd(seq(-20, 20, 1)) # If SD of
# Y falls outside the window of +/- 20 positions SD is assumed to be equal
# to that of the uniform distribution
pos_err_comb <- sqrt(Ypos_stat[, 2] ^ 2 + Xpos_stat[, 2] ^ 2) # Find
# combined error on position (day)
Xpos_minmax <- cbind(Xpos_stat[, 1] - pos_err_comb, Xpos_stat[, 1] +
pos_err_comb) %% length(D) # Find min and max position values
Xminmax <- matrix(approx(x = X, xout = Xpos_minmax, rule = 2)$y, ncol = 2)
# Find associated D values (linear interpolation)
x_err_comb <- 0.5 * ((Xminmax[,2] - Xminmax[,1]) %% X[length(X)]) # Find
# combined SD in D domain
return(x_err_comb)
}
mc_err_proj <- function(x, x_err, y, y_err, X, Y, MC){ # Function to propagate
# combined errors on x and y on the modelled X and Y values by means of local
# projection of y uncertainty on x and subsequent combination of uncertainties
# in X domain
dYdX <- diff(Y[, 2]) / diff(X) # Create first derivative of Y by X
dYdX[which(abs(dYdX) <= 10 ^ (floor(log(mean(abs(dYdX)), 10)) - 1))] <-
sign(dYdX[which(abs(dYdX) <= 10 ^ (floor(log(mean(abs(dYdX)),
10)) - 1))]) * 10 ^ (floor(log(mean(abs(dYdX)), 10)) - 1) # Remove
# small absolute values (more than one order of magnitude smaller
# than the mean), preserving their sign
dYdX <- append(dYdX, dYdX[length(dYdX)]) # repeat last value to increase the
# length of the vector to match X/Y (366 days)
localslope <- dYdX[apply(abs(outer(x, X, FUN = "-")), 1, which.min)] # Find
# the local slope belonging to each sample position
x_err_proj <- y_err / localslope # Project uncertainty on Y on x domain
# using slope
x_err_comb <- sqrt(x_err_proj ^2 + x_err ^2) # Combine uncertainties on x
# and y in the X domain
return(x_err_comb)
}
View(d18O_model)
sinreg <- function(x, # Function to perform sinusoid regression meant to
# estimate the starting parameters for fitting growth and temperature sinusoids
y,
plot = FALSE # Plot results?
){
Ots <- ts(y) # Turn y into a time series
ssp <- spectrum(Ots, plot = FALSE) # Create periodogram of y
Nper <- 1 / ssp$freq[ssp$spec == max(ssp$spec)] # Estimate period in terms
# of sample number
Dper <- Nper * diff(range(x)) / length(x) # Convert period to depth domain
sinlm <- lm(y ~ sin(2 * pi / Dper * x) + cos(2 * pi / Dper * x)) # Run
# linear model, cutting up d18O = Asin(2 * pi * D) into
# d18O = asin(D) + bcos(D)
sinm <- sinlm$fitted # Extract model result
if(plot == TRUE){
dev.new()
plot(x, y); lines(x, sinm, col = "red")
}
coeff <- summary(sinlm)[["coefficients"]] # Extract a, b and intercept +
# uncertainties
# Calculate coefficients of the form d18O = I + Asin(2 * pi * D + p)
I <- coeff[1, 1] # Intercept (mean annual value)
A <- sqrt(coeff[2, 1] ^ 2 + coeff[3, 1] ^ 2) # Amplitude of seasonality
phase <- -acos(coeff[2, 1] / A) # Phase of sinusoid
peak <- (0.25 + (phase / (2 * pi))) * Dper # timing of seasonal peak
R2adj <- summary(sinlm)$adj.r.squared # Goodness of model fit (adjusted R2)
p <- as.numeric(pf(summary(sinlm)$fstatistic[1],
summary(sinlm)$fstatistic[2],
summary(sinlm)$fstatistic[3],
lower.tail = F)) # p-value of model
return(list(c(I, A, Dper, peak, R2adj, p), sinm)) # Return results of
# regression
}
devtools::document()
devtools::document()
devtools::document()
devtools::document()
devtools::document()
devtools::install()
devtools::check()
devtools::check()
devtools::check()
devtools::document()
devtools::install()
devtools::document()
devtools::document()
warnings()
devtools::document()
devtools::document()
devtools::document()
devtools::document()
devtools::install()
