D47m_poly_result <- cbind(10^6 / (newdat + 273.15) ^2, D47m_poly_pred$fit)
# Polynomial regression with errors on D47 and Temp using MC simulation
D47m_poly_MC <- lm(D47 ~ poly(x, 3), data = violin_data, subset = which(violin_data$outlier == FALSE))
D47m_poly_MC_pred <- predict.lm(D47m_poly_MC, newdata = newdat_York, se.fit = TRUE, interval = "confidence", level = 0.95)
D47m_poly_MC_result <- cbind(newdat_York, D47m_poly_MC_pred$fit)
# Readjust 95% CL calculations for actual degrees of freedom
D47m_poly_MC_result$lwr <- D47m_poly_MC_result$fit - (D47m_poly_MC_result$fit - D47m_poly_MC_result$lwr) * sqrt(D47m_poly_MC$df.residual) / sqrt(length(violin_data$D47) / Nsim - 4)
D47m_poly_MC_result$upr <- D47m_poly_MC_result$fit + (D47m_poly_MC_result$upr - D47m_poly_MC_result$fit) * sqrt(D47m_poly_MC$df.residual) / sqrt(length(violin_data$D47) / Nsim - 4)
# -----------------------Add Preexisting calibrations---------------------------
# Add dummy data to plot Anderson calibration
Anderson21 <- data.frame(Temp = 10 ^ 6 / (seq(0, 1000, 0.1) + 273.15) ^ 2,
D47 = 0.0391 * 10 ^ 6 / (seq(0, 1000, 0.1) + 273.15) ^ 2 + 0.154) # Latest Anderson calibration
# Add dummy data to plot Meinicke calibration
MeinickeICDES <- data.frame(Temp = 10 ^ 6 / (seq(0, 1000, 0.1) + 273.15) ^ 2,
D47 = 0.0397 * 10 ^ 6 / (seq(0, 1000, 0.1) + 273.15) ^ 2 + 0.1518) # Recalculated Meinicke calibration
# Add theoretical calcite and aragonite calibration lines by Guo et al. 2009 recalculated to the I-CDES scale
Guo09 <- data.frame(Temp = 10 ^ 6 / (seq(0, 1000, 0.1) + 273.15) ^ 2,
D47_cc_original = -3.33040 * 10 ^ 9 / (seq(0, 1000, 0.1) + 273.15) ^ 4 + 2.32415 * 10 ^ 7 / (seq(0, 1000, 0.1) + 273.15) ^ 3 - 2.91282 * 10 ^ 3 / (seq(0, 1000, 0.1) + 273.15) ^ 2 - 5.54042 / (seq(0, 1000, 0.1) + 273.15), # Original formula published by Guo et al., 2009
D47_ar_original = -3.43068 * 10 ^ 9 / (seq(0, 1000, 0.1) + 273.15) ^ 4 + 2.35766 * 10 ^ 7 / (seq(0, 1000, 0.1) + 273.15) ^ 3 - 8.06003 * 10 ^ 2 / (seq(0, 1000, 0.1) + 273.15) ^ 2 - 6.90300 / (seq(0, 1000, 0.1) + 273.15), # Original formula published by Guo et al., 2009
D47_cc_CDES25 = -3.33040 * 10 ^ 9 / (seq(0, 1000, 0.1) + 273.15) ^ 4 + 2.32415 * 10 ^ 7 / (seq(0, 1000, 0.1) + 273.15) ^ 3 - 2.91282 * 10 ^ 3 / (seq(0, 1000, 0.1) + 273.15) ^ 2 - 5.54042 / (seq(0, 1000, 0.1) + 273.15) + 0.23252 + 0.268 - 0.232, # Corrected to CDES reference fram by updating the D47-D63 fractionation factor from 0.232 to 0.268
D47_ar_CDES25 = -3.43068 * 10 ^ 9 / (seq(0, 1000, 0.1) + 273.15) ^ 4 + 2.35766 * 10 ^ 7 / (seq(0, 1000, 0.1) + 273.15) ^ 3 - 8.06003 * 10 ^ 2 / (seq(0, 1000, 0.1) + 273.15) ^ 2 - 6.90300 / (seq(0, 1000, 0.1) + 273.15) + 0.22893 + 0.268 - 0.232, # Corrected to CDES reference fram by updating the D47-D63 fractionation factor from 0.232 to 0.268
D47_cc_CDES90 = -3.33040 * 10 ^ 9 / (seq(0, 1000, 0.1) + 273.15) ^ 4 + 2.32415 * 10 ^ 7 / (seq(0, 1000, 0.1) + 273.15) ^ 3 - 2.91282 * 10 ^ 3 / (seq(0, 1000, 0.1) + 273.15) ^ 2 - 5.54042 / (seq(0, 1000, 0.1) + 273.15) + 0.23252 + 0.268 - 0.232 - 0.088, # Bring to CDES90 by applying the 25-70 degrees acid fractionation factor by Petersen et al., 2019
D47_ar_CDES90 = -3.43068 * 10 ^ 9 / (seq(0, 1000, 0.1) + 273.15) ^ 4 + 2.35766 * 10 ^ 7 / (seq(0, 1000, 0.1) + 273.15) ^ 3 - 8.06003 * 10 ^ 2 / (seq(0, 1000, 0.1) + 273.15) ^ 2 - 6.90300 / (seq(0, 1000, 0.1) + 273.15) + 0.22893 + 0.268 - 0.232 - 0.088, # Bring to CDES90 by applying the 25-70 degrees acid fractionation factor by Petersen et al., 2019
D47_cc_CDES90_corr = (-3.33040 * 10 ^ 9 / (seq(0, 1000, 0.1) + 273.15) ^ 4 + 2.32415 * 10 ^ 7 / (seq(0, 1000, 0.1) + 273.15) ^ 3 - 2.91282 * 10 ^ 3 / (seq(0, 1000, 0.1) + 273.15) ^ 2 - 5.54042 / (seq(0, 1000, 0.1) + 273.15) + 0.23252 + 0.268 - 0.232 - 0.088) * 1.035, # Correct for D47-dependent D47-D63 fractionation factor of 35 ppm/per mille found by Guo et al. 2009 (and implemented in Jautzy et al., 2020)
D47_ar_CDES90_corr = (-3.43068 * 10 ^ 9 / (seq(0, 1000, 0.1) + 273.15) ^ 4 + 2.35766 * 10 ^ 7 / (seq(0, 1000, 0.1) + 273.15) ^ 3 - 8.06003 * 10 ^ 2 / (seq(0, 1000, 0.1) + 273.15) ^ 2 - 6.90300 / (seq(0, 1000, 0.1) + 273.15) + 0.22893 + 0.268 - 0.232 - 0.088) * 1.035 # Correct for D47-dependent D47-D63 fractionation factor of 35 ppm/per mille found by Guo et al. 2009 (and implemented in Jautzy et al., 2020)
)
# Values used to put Guo et al. 2009 data on the I-CDES scale
# ETH-1 - Iso A (Meckler et al., 2014) = 600 degr with D47 of 0.2052
# ETH-2 - Iso B (Meckler et al., 2014) = 600 degr with D47 of 0.2085
# ETH-3 - Iso C (Meckler et al., 2014) = ~20 degr with D47 of 0.6132
ETH1_GuoCDES <- Guo09$D47_ar_CDES25[which(Guo09$Temp == 10 ^ 6 / (600 + 273.15) ^ 2)]
ETH3_GuoCDES <- Guo09$D47_ar_CDES25[which(Guo09$Temp == 10 ^ 6 / (20 + 273.15) ^ 2)]
ETH1_ICDES <- 0.2052
ETH3_ICDES <- 0.6132
# Place on I-CDES scale
Guo09$D47_cc_ICDES <- (Guo09$D47_cc_CDES25 - ETH1_GuoCDES) * (ETH1_ICDES-ETH3_ICDES)/(ETH1_GuoCDES-ETH3_GuoCDES) + ETH1_ICDES # Use linear correction through ETH1 and ETH3 values to transform to I-CDES scale
Guo09$D47_ar_ICDES <- (Guo09$D47_ar_CDES25 - ETH1_GuoCDES) * (ETH1_ICDES-ETH3_ICDES)/(ETH1_GuoCDES-ETH3_GuoCDES) + ETH1_ICDES # Use linear correction through ETH1 and ETH3 values to transform to I-CDES scale
Guo09$D47_cc <- Guo09$D47_cc_ICDES * 1.035 # Correct for D47-dependent D47-D63 fractionation factor of 35 ppm/per mille found by Guo et al. 2009 (and implemented in Jautzy et al., 2020)
Guo09$D47_ar <- Guo09$D47_ar_ICDES * 1.035 # Correct for D47-dependent D47-D63 fractionation factor of 35 ppm/per mille found by Guo et al. 2009 (and implemented in Jautzy et al., 2020)
View(D47m_York_result)
View(D47m_lowT_York_result)
D47m_lowT_York_result$Diff_D47m_York <- D47m_lowT_York_result$fit - D47m_York_result$fit[1:1001]
plot(D47m_lowT_York_result$Diff_D47m_York)
D47m_lowT_York_result$err <- D47m_lowT_York_result$fit - D47m_lowT_York_result$lwr
D47m_York_result$err <- D47m_York_result$fit - D47m_York_result$lwr
View(Temperature_offset)
View(Temperature_offset)
Temperature_offset(0.68755, 0.0052, 0, 0.0449, 0.0291)
Temperature_offset(0.68755, 0.005, 0, 0.0449, 0.0291)
Temperature_offset(0.6925, 0.005, 0, 0.0449, 0.0291)
Temperature_offset(0.6925, 0.005, 0, 0.0450, 0.0896)
Temperature_offset(0.4128477, 0.010, 0, 0.0450, 0.0896)
Temperature_offset(0.4128477, 0.00399, 0, 0.0450, 0.0896)
# -------------------------Regression residuals---------------------------------
# Extract D47 values and their errors for temperatures of all samples
newdat_York <- data.frame(x = 10 ^6 / (dat$Temp + 273.15) ^ 2)
dat$D47res_York <- dat$D47 - predict(D47m_York, newdata = newdat_York, se.fit = TRUE, interval = "none", level = 0.95)$fit
dat$D47res_lowT_York <- dat$D47 - predict(D47m_lowT_York, newdata = newdat_York, se.fit = TRUE, interval = "none", level = 0.95)$fit
dat$D47res_poly <- dat$D47 - predict(D47m_poly_MC, newdata = newdat_York, se.fit = TRUE, interval = "none", level = 0.95)$fit
dat$D47res_Anderson <- dat$D47 - (0.0391 * 10 ^ 6 / (dat$Temp + 273.15) ^ 2 + 0.154)
dat$D47res_Meinicke <- dat$D47 - (0.0397 * 10 ^ 6 / (dat$Temp + 273.15) ^ 2 + 0.1518)
dat$D47res_Guo_cc <- dat$D47 - (((-3.33040 * 10 ^ 9 / (dat$Temp + 273.15) ^ 4 + 2.32415 * 10 ^ 7 / (dat$Temp + 273.15) ^ 3 - 2.91282 * 10 ^ 3 / (dat$Temp + 273.15) ^ 2 - 5.54042 / (dat$Temp + 273.15) + 0.23252 + 0.268 - 0.232) - ETH1_GuoCDES) * (ETH1_ICDES-ETH3_ICDES)/(ETH1_GuoCDES-ETH3_GuoCDES) + ETH1_ICDES) * 1.035
dat$D47res_Guo_ar <- dat$D47 - (((-3.43068 * 10 ^ 9 / (dat$Temp + 273.15) ^ 4 + 2.35766 * 10 ^ 7 / (dat$Temp + 273.15) ^ 3 - 8.06003 * 10 ^ 2 / (dat$Temp + 273.15) ^ 2 - 6.90300 / (dat$Temp + 273.15) + 0.22893 + 0.268 - 0.232) - ETH1_GuoCDES) * (ETH1_ICDES-ETH3_ICDES)/(ETH1_GuoCDES-ETH3_GuoCDES) + ETH1_ICDES) * 1.035
newdat_York <- data.frame(x = 10 ^6 / (D47stats$Temp + 273.15) ^ 2)
D47stats$D47res_York <- D47stats$D47_mean - predict(D47m_York, newdata = newdat_York, se.fit = TRUE, interval = "none", level = 0.95)$fit
D47stats$D47res_lowT_York <- D47stats$D47_mean - predict(D47m_lowT_York, newdata = newdat_York, se.fit = TRUE, interval = "none", level = 0.95)$fit
D47stats$D47res_poly <- D47stats$D47_mean - predict(D47m_poly_MC, newdata = newdat_York, se.fit = TRUE, interval = "none", level = 0.95)$fit
D47stats$D47res_Anderson <- D47stats$D47_mean - (0.0391 * 10 ^ 6 / (D47stats$Temp + 273.15) ^ 2 + 0.154)
D47stats$D47res_Meinicke <- D47stats$D47_mean - (0.0397 * 10 ^ 6 / (D47stats$Temp + 273.15) ^ 2 + 0.1518)
D47stats$D47res_Guo_cc <- D47stats$D47_mean - (((-3.33040 * 10 ^ 9 / (D47stats$Temp + 273.15) ^ 4 + 2.32415 * 10 ^ 7 / (D47stats$Temp + 273.15) ^ 3 - 2.91282 * 10 ^ 3 / (D47stats$Temp + 273.15) ^ 2 - 5.54042 / (D47stats$Temp + 273.15) + 0.23252 + 0.268 - 0.232) - ETH1_GuoCDES) * (ETH1_ICDES-ETH3_ICDES)/(ETH1_GuoCDES-ETH3_GuoCDES) + ETH1_ICDES) * 1.035
D47stats$D47res_Guo_ar <- D47stats$D47_mean - (((-3.43068 * 10 ^ 9 / (D47stats$Temp + 273.15) ^ 4 + 2.35766 * 10 ^ 7 / (D47stats$Temp + 273.15) ^ 3 - 8.06003 * 10 ^ 2 / (D47stats$Temp + 273.15) ^ 2 - 6.90300 / (D47stats$Temp + 273.15) + 0.22893 + 0.268 - 0.232) - ETH1_GuoCDES) * (ETH1_ICDES-ETH3_ICDES)/(ETH1_GuoCDES-ETH3_GuoCDES) + ETH1_ICDES) * 1.035
newdat_York <- data.frame(x = 10 ^6 / (Aisstats$Temp + 273.15) ^ 2)
Aisstats$D47res_York <- Aisstats$D47_mean - predict(D47m_York, newdata = newdat_York, se.fit = TRUE, interval = "none", level = 0.95)$fit
Aisstats$D47res_lowT_York <- Aisstats$D47_mean - predict(D47m_lowT_York, newdata = newdat_York, se.fit = TRUE, interval = "none", level = 0.95)$fit
Aisstats$D47res_poly <- Aisstats$D47_mean - predict(D47m_poly_MC, newdata = newdat_York, se.fit = TRUE, interval = "none", level = 0.95)$fit
Aisstats$D47res_Anderson <- Aisstats$D47_mean - (0.0391 * 10 ^ 6 / (Aisstats$Temp + 273.15) ^ 2 + 0.154)
Aisstats$D47res_Meinicke <- Aisstats$D47_mean - (0.0397 * 10 ^ 6 / (Aisstats$Temp + 273.15) ^ 2 + 0.1518)
Aisstats$D47res_Guo_cc <- Aisstats$D47_mean - (((-3.33040 * 10 ^ 9 / (Aisstats$Temp + 273.15) ^ 4 + 2.32415 * 10 ^ 7 / (Aisstats$Temp + 273.15) ^ 3 - 2.91282 * 10 ^ 3 / (Aisstats$Temp + 273.15) ^ 2 - 5.54042 / (Aisstats$Temp + 273.15) + 0.23252 + 0.268 - 0.232) - ETH1_GuoCDES) * (ETH1_ICDES-ETH3_ICDES)/(ETH1_GuoCDES-ETH3_GuoCDES) + ETH1_ICDES) * 1.035
Aisstats$D47res_Guo_ar <- Aisstats$D47_mean - (((-3.43068 * 10 ^ 9 / (Aisstats$Temp + 273.15) ^ 4 + 2.35766 * 10 ^ 7 / (Aisstats$Temp + 273.15) ^ 3 - 8.06003 * 10 ^ 2 / (Aisstats$Temp + 273.15) ^ 2 - 6.90300 / (Aisstats$Temp + 273.15) + 0.22893 + 0.268 - 0.232) - ETH1_GuoCDES) * (ETH1_ICDES-ETH3_ICDES)/(ETH1_GuoCDES-ETH3_GuoCDES) + ETH1_ICDES) * 1.035
newdat_York <- data.frame(x = 10 ^6 / (violin_data$Temp + 273.15) ^ 2)
violin_data$D47res_York <- violin_data$D47 - predict(D47m_York, newdata = newdat_York, se.fit = TRUE, interval = "none", level = 0.95)$fit
violin_data$D47res_lowT_York <- violin_data$D47 - predict(D47m_lowT_York, newdata = newdat_York, se.fit = TRUE, interval = "none", level = 0.95)$fit
violin_data$D47res_poly <- violin_data$D47 - predict(D47m_poly_MC, newdata = newdat_York, se.fit = TRUE, interval = "none", level = 0.95)$fit
violin_data$D47res_Anderson <- violin_data$D47 - (0.0391 * 10 ^ 6 / (violin_data$Temp + 273.15) ^ 2 + 0.154)
violin_data$D47res_Meinicke <- violin_data$D47 - (0.0397 * 10 ^ 6 / (violin_data$Temp + 273.15) ^ 2 + 0.1518)
violin_data$D47res_Guo_cc <- violin_data$D47 - (((-3.33040 * 10 ^ 9 / (violin_data$Temp + 273.15) ^ 4 + 2.32415 * 10 ^ 7 / (violin_data$Temp + 273.15) ^ 3 - 2.91282 * 10 ^ 3 / (violin_data$Temp + 273.15) ^ 2 - 5.54042 / (violin_data$Temp + 273.15) + 0.23252 + 0.268 - 0.232) - ETH1_GuoCDES) * (ETH1_ICDES-ETH3_ICDES)/(ETH1_GuoCDES-ETH3_GuoCDES) + ETH1_ICDES) * 1.035
violin_data$D47res_Guo_ar <- violin_data$D47 - (((-3.43068 * 10 ^ 9 / (violin_data$Temp + 273.15) ^ 4 + 2.35766 * 10 ^ 7 / (violin_data$Temp + 273.15) ^ 3 - 8.06003 * 10 ^ 2 / (violin_data$Temp + 273.15) ^ 2 - 6.90300 / (violin_data$Temp + 273.15) + 0.22893 + 0.268 - 0.232) - ETH1_GuoCDES) * (ETH1_ICDES-ETH3_ICDES)/(ETH1_GuoCDES-ETH3_GuoCDES) + ETH1_ICDES) * 1.035
# Calculate values for calibrations relative to regressions through our dataset
D47m_York_result_res <- D47m_York_result - D47m_York_result$fit
D47m_York_result_res$x <- D47m_York_result_res$x + D47m_York_result$fit
D47m_York_result_res$Anderson <- Anderson21$D47 - D47m_York_result$fit
D47m_York_result_res$Meinicke <- MeinickeICDES$D47 - D47m_York_result$fit
D47m_York_result_res$Guo_cc <- Guo09$D47_cc - D47m_York_result$fit
D47m_York_result_res$Guo_ar <- Guo09$D47_ar - D47m_York_result$fit
D47m_lowT_York_result_res <- D47m_lowT_York_result - D47m_lowT_York_result$fit
D47m_lowT_York_result_res$x <- D47m_lowT_York_result_res$x + D47m_lowT_York_result$fit
D47m_lowT_York_result_res$Anderson <- Anderson21$D47[1:1001] - D47m_lowT_York_result$fit
D47m_lowT_York_result_res$Meinicke <- MeinickeICDES$D47[1:1001] - D47m_lowT_York_result$fit
D47m_lowT_York_result_res$Guo_cc <- Guo09$D47_cc[1:1001] - D47m_lowT_York_result$fit
D47m_lowT_York_result_res$Guo_ar <- Guo09$D47_ar[1:1001] - D47m_lowT_York_result$fit
D47m_poly_MC_result_res <- D47m_poly_MC_result - D47m_poly_MC_result$fit
D47m_poly_MC_result_res$x <- D47m_poly_MC_result_res$x + D47m_poly_MC_result$fit
D47m_poly_MC_result_res$Anderson <- Anderson21$D47 - D47m_poly_MC_result$fit
D47m_poly_MC_result_res$Meinicke <- MeinickeICDES$D47 - D47m_poly_MC_result$fit
D47m_poly_MC_result_res$Guo_cc <- Guo09$D47_cc - D47m_poly_MC_result$fit
D47m_poly_MC_result_res$Guo_ar <- Guo09$D47_ar - D47m_poly_MC_result$fit
# Prepare summary of calibration offsets
calibration_offset <- data.frame(D47_offset = c(dat$D47res_Anderson[which(dat$type == "bivalve" & (dat$Analysis == "this study" | dat$Analysis == "Bernasconi18"))],
dat$D47res_Meinicke[which(dat$type == "bivalve" & (dat$Analysis == "this study" | dat$Analysis == "Bernasconi18"))],
dat$D47res_Guo_cc[which(dat$type == "bivalve" & (dat$Analysis == "this study" | dat$Analysis == "Bernasconi18"))],
dat$D47res_Guo_ar[which(dat$type == "bivalve" & (dat$Analysis == "this study" | dat$Analysis == "Bernasconi18"))]),
D47_SD = rep(dat$D47_SD[which(dat$type == "bivalve" & (dat$Analysis == "this study" | dat$Analysis == "Bernasconi18"))], 4),
Calibration = c(rep("Anderson et al., 2021", length(dat$D47res_Meinicke[which(dat$type == "bivalve" & (dat$Analysis == "this study" | dat$Analysis == "Bernasconi18"))])),
rep("Meinicke et al., 2020", length(dat$D47res_Meinicke[which(dat$type == "bivalve" & (dat$Analysis == "this study" | dat$Analysis == "Bernasconi18"))])),
rep("Guo et al., 2009 (calcite)", length(dat$D47res_Meinicke[which(dat$type == "bivalve" & (dat$Analysis == "this study" | dat$Analysis == "Bernasconi18"))])),
rep("Guo et al., 2009 (aragonite)", length(dat$D47res_Meinicke[which(dat$type == "bivalve" & (dat$Analysis == "this study" | dat$Analysis == "Bernasconi18"))])))
)
# Calculate the temperature equivalent of the calibration offset
mean_Temp <- mean(dat$Temp[which(dat$type == "bivalve" & (dat$Analysis == "this study" | dat$Analysis == "Bernasconi18"))])
mean_D47 <- 0.0391 * 10 ^ 6 / (mean_Temp + 273.13) ^ 2 + 0.154
# Statistics of difference with calibration
Calibration_offset_stats <- calibration_offset %>% # Summarize D47 offset statistics
group_by(Calibration) %>%
summarize(
N = n(), # Calculate the number of modelled values, excluding NA's
D47_offset_Average = binmeans(x = D47_offset, x_sd = D47_SD, output = "mean"),
D47_offset_SD = binmeans(x = D47_offset, x_sd = D47_SD, output = "SD"),
D47_offset_SE = D47_offset_SD / sqrt(N - 1), # Calculate the standard error
D47_offset_CL95 = qt(0.95, N - 1) * D47_offset_SE # Calculate the 95% confidence level
) %>%
ungroup()
Calibration_offset_stats$Temp_offset_Average <- sqrt(0.0391 * 10 ^ 6 / (mean_D47 - Calibration_offset_stats$D47_offset_Average - 0.154)) - 273.15 - mean_Temp
Calibration_offset_stats$Temp_offset_SD <- sqrt(0.0391 * 10 ^ 6 / (mean_D47 - Calibration_offset_stats$D47_offset_Average - Calibration_offset_stats$D47_offset_SD - 0.154)) - 273.15 - mean_Temp - Calibration_offset_stats$Temp_offset_Average
Calibration_offset_stats$Temp_offset_SE <- sqrt(0.0391 * 10 ^ 6 / (mean_D47 - Calibration_offset_stats$D47_offset_Average - Calibration_offset_stats$D47_offset_SE - 0.154)) - 273.15 - mean_Temp - Calibration_offset_stats$Temp_offset_Average
Calibration_offset_stats$Temp_offset_CL95 <- sqrt(0.0391 * 10 ^ 6 / (mean_D47 - Calibration_offset_stats$D47_offset_Average - Calibration_offset_stats$D47_offset_CL95 - 0.154)) - 273.15 - mean_Temp - Calibration_offset_stats$Temp_offset_Average
View(calibration_offset)
View(dat)
mean(dat$D47res_Anderson)
sd(dat$D47res_Anderson)
sd(dat$D47res_Anderson)/sqrt(length(dat$D47res_Anderson) - 1)
qt(0.05, length(dat$D47res_Anderson))
qt(0.95, length(dat$D47res_Anderson))
qt(0.95, length(dat$D47res_Anderson)) * sd(dat$D47res_Anderson)/sqrt(length(dat$D47res_Anderson) - 1)
mean(dat$D47res_Anderson[-which(dat$Analysis == "Muller17")])
qt(0.95, length(dat$D47res_Anderson[-which(dat$Analysis == "Muller17")])) * sd(dat$D47res_Anderson[-which(dat$Analysis == "Muller17")])/sqrt(length(dat$D47res_Anderson[-which(dat$Analysis == "Muller17")]) - 1)
View(Calibration_offset_stats)
load("C:/Users/niels/Dropbox/Research/postdoc/UNBIAS/growth experiments/LAICPMS_Sr_spiking/Batch2/LA data/LA_combined_batch2.Rdata")
View(LA_combined)
Profile_plot_Sr_offset_all <- ggplot(LA_combined) +
geom_point(data = subset(LA_combined, !(Specimen_id %in% c(5, 7, 9))),
aes(Depth,
SrCa + as.numeric(Specimen_id) - 1,
col = Species),
alpha = 0.1,
size = 0.1) +
scale_y_continuous("[Sr]/[Ca] (mmol/mol)",
breaks = seq(0, 20, 1),
labels = seq(0, 20, 1),
limits = c(0, 20)) +
scale_x_continuous("Distance from ventral margin [mm]") +
ggtitle("Offset (+ 1) Sr/Ca curves") +
theme_bw()
require(tidyverse)
require(gridExtra)
Profile_plot_Sr_offset_all <- ggplot(LA_combined) +
geom_point(data = subset(LA_combined, !(Specimen_id %in% c(5, 7, 9))),
aes(Depth,
SrCa + as.numeric(Specimen_id) - 1,
col = Species),
alpha = 0.1,
size = 0.1) +
scale_y_continuous("[Sr]/[Ca] (mmol/mol)",
breaks = seq(0, 20, 1),
labels = seq(0, 20, 1),
limits = c(0, 20)) +
scale_x_continuous("Distance from ventral margin [mm]") +
ggtitle("Offset (+ 1) Sr/Ca curves") +
theme_bw()
x11(); plot(Profile_plot_Sr_offset_all)
Profile_plot_Sr_offset_all <- ggplot(LA_combined) +
geom_point(data = subset(LA_combined, !(Specimen_id %in% c(5, 7, 9))),
aes(Depth,
SrCa + as.numeric(Specimen_id) - 1,
col = Species),
alpha = 0.1,
size = 0.1) +
geom_smooth() +
scale_y_continuous("[Sr]/[Ca] (mmol/mol)",
breaks = seq(0, 20, 1),
labels = seq(0, 20, 1),
limits = c(0, 20)) +
scale_x_continuous("Distance from ventral margin [mm]") +
ggtitle("Offset (+ 1) Sr/Ca curves") +
theme_bw()
plot(Profile_plot_Sr_offset_all)
Profile_plot_Sr_offset_all <- ggplot(LA_combined) +
geom_point(data = subset(LA_combined, !(Specimen_id %in% c(5, 7, 9))),
aes(Depth,
SrCa + as.numeric(Specimen_id) - 1,
col = Species),
alpha = 0.1,
size = 0.1) +
geom_smooth(aes(x = Depth,
y = SrCa + as.numeric(Specimen_id) - 1,
col = Species)) +
scale_y_continuous("[Sr]/[Ca] (mmol/mol)",
breaks = seq(0, 20, 1),
labels = seq(0, 20, 1),
limits = c(0, 20)) +
scale_x_continuous("Distance from ventral margin [mm]") +
ggtitle("Offset (+ 1) Sr/Ca curves") +
theme_bw()
plot(Profile_plot_Sr_offset_all)
?geom_smooth
plot(LA_combined$Distance)
plot(LA_combined$Xpos, LA_combined$Ypos)
x11(); plot(LA_combined$Xpos, LA_combined$Ypos)
read.csv("inst/extdata/Virtual_shell.csv")
dat <- read.csv("inst/extdata/Virtual_shell.csv")
?duplicated
duplicated(dat$D)
all(duplicated(dat$D))
all(c("TRUE", "TRUE", "FALSE"))
all(c("TRUE", "TRUE", "TRUE"))
TRUE in duplicated(dat$D)
(TRUE in duplicated(dat$D))
(TRUE %in% duplicated(dat$D))
(TRUE %in% c("TRUE", "TRUE", "FALSE"))
which("TRUE", "TRUE", "FALSE")
which(c("TRUE", "TRUE", "FALSE"))
which(c("TRUE", "TRUE", "FALSE") == TRUE)
which(duplicated(dat$D) == TRUE)
length(duplicated(dat$D))
length(dat$D)
test <- data.frame(D = c(0, 1, 2, 3, 3, 4, 5, 6, 6, 10), d18O = 1:10)
duplicated(test$D)
dat <- dat[-which(duplicated(dat$D) == TRUE), ]
dat <- read.csv("inst/extdata/Virtual_shell.csv")
?data,import
?data_import
require(ShellChron)
?data_import
data_import("inst/extdata/Virtual_shell.csv")
import_list <- data_import("inst/extdata/Virtual_shell.csv")
?getwd()
getwd()
import_list <- data_import("E:/Dropbox/Research/Manuscripts/[2022Published] GMD - Bivalve age model/tests/Virtual_shell_duplicates.csv")
import_list <- data_import("inst/extdata/Virtual_shell.csv")
file_name <- "E:/Dropbox/Research/Manuscripts/[2022Published] GMD - Bivalve age model/tests/Virtual_shell_duplicates.csv"
dat <- read.csv(file_name, header = T)
dat <- dat[order(dat[, 1]),] # Order data by D
# Check the structure of the import dataframe
if(ncol(dat) == 5){ # If the number of columns checks out
# Check the column names, and rename them if necessary
if(!all(colnames(dat) == c("D", "d18Oc", "YEARMARKER", "D_err", "d18Oc_err"))){
colnames(dat) <- c("D", "d18Oc", "YEARMARKER", "D_err", "d18Oc_err")
}
}else if(ncol(dat) == 3){ # If SD columns are omitted
# Check the names of provided columns, and rename them if necessary
if(!all(colnames(dat) == c("D", "d18Oc", "YEARMARKER"))){
colnames(dat) <- c("D", "d18Oc", "YEARMARKER")
}
dat$D_err <- rep(0, nrow(dat))
dat$d18Oc_err <- rep(0, nrow(dat))
}else{
return("ERROR: Input data does not match the default input data format")
}
# Check for duplicate depth values
if(TRUE %in% duplicated(dat$D)){
dat <- dat[-which(duplicated(dat$D) == TRUE), ] # Remove duplicated depth values
print("WARNING: Duplicated depth values were found and removed")
}
# Define sliding window based on indicated year markers
YEARMARKER <- which(dat$YEARMARKER == 1) # Read position of yearmarkers in data.
yearwindow <- diff(which(dat$YEARMARKER == 1)) # Calculate the number of datapoints in each year between consecutive year markers
if(length(yearwindow) > 1){
dynwindow <- approx( # Interpolate between the numbers of annual datapoints to create list of starting positions of growth windows and their size for age modeling
x = YEARMARKER[-length(YEARMARKER)],
y = yearwindow,
xout = 1:(length(dat$D) - yearwindow[length(yearwindow)] + 1), # X indicates starting positions of windows used for age modeling
method = "linear",
rule = 2 # Window sizes for beginning given as NA's, for end equal to last value
)
dynwindow$y <- round(dynwindow$y) # Round off window sizes to integers
dynwindow$y[dynwindow$y < 10] <- 10 # Eliminate small window sizes to lend confidence to the sinusoidal fit
overshoot<-which(dynwindow$x + dynwindow$y > length(dat[,1])) # Find windows that overshoot the length of dat
dynwindow$x <- dynwindow$x[-overshoot] # Remove overshooting windows
dynwindow$y <- dynwindow$y[-overshoot] # Remove overshooting windows
if((length(dynwindow$x) + dynwindow$y[length(dynwindow$x)] - 1) < length(dat[, 1])){ # Increase length of the final window in case samples at the end are missed due to jumps in window size
dynwindow$y[length(dynwindow$y)] <- dynwindow$y[length(dynwindow$y)] + (length(dat[, 1]) - (length(dynwindow$x) + dynwindow$y[length(dynwindow$x)] - 1))
}
}else if(length(yearwindow) == 1){ # Catch exception of datasets with only two yearmarkers
dynwindow <- data.frame(
x = 1:(length(dat$D) - yearwindow[length(yearwindow)] + 1),
y = rep(yearwindow, (length(dat$D) - yearwindow[length(yearwindow)] + 1))
)
}else{
return("ERROR: Need at least 2 year markers to estimate window size")
}
devtools::install_github("nielsjdewinter/ShellChron")
require(ShellChron)
?wrap_function
getwd()
wrap_function(path = "E:/Dropbox/Research/Manuscripts/[2022Published] GMD - Bivalve age model/tests",
file_name = "Virtual_shell_duplicates.csv",
transfer_function = "KimONeil97",
t_int = 1,
T_per = 365,
d18Ow = 0,
t_maxtemp = 183,
sinfit = TRUE,
MC = 1000,
plot = TRUE,
plot_export = TRUE,
export_raw = TRUE,
export_path = "E:/Dropbox/Research/Manuscripts/[2022Published] GMD - Bivalve age model/tests/test_results"
)
devtools::document()
devtools::install()
devtools::document()
devtools::install()
require(ShellChron
)
require(ShellChron)
dat <- as.data.frame(seq(1000, 40000, 1000))
colnames(dat) <- "D"
dat$d18Oc <- sin((2 * pi * (seq(1, 40, 1) - 8 + 7 / 4)) / 7)
dat$YEARMARKER <- c(0, rep(c(0, 0, 0, 0, 0, 0, 1), 5), 0, 0, 0, 0)
dat$D_err <- rep(100, 40)
dat$d18Oc_err <- rep(0.1, 40)
importlist <- data_import_object(object_name = dat)
devtools::install()
require(ShellChron)
dat <- as.data.frame(seq(1000, 40000, 1000))
colnames(dat) <- "D"
dat$d18Oc <- sin((2 * pi * (seq(1, 40, 1) - 8 + 7 / 4)) / 7)
dat$YEARMARKER <- c(0, rep(c(0, 0, 0, 0, 0, 0, 1), 5), 0, 0, 0, 0)
dat$D_err <- rep(100, 40)
dat$d18Oc_err <- rep(0.1, 40)
importlist <- data_import_object(object_name = dat)
dat <- importlist[[1]]
dynwindow <- importlist[[2]]
View(dat)
devtools::document()
devtools::install()
wrap_function(
path = "E:/Dropbox/Research/postdoc/UNBIAS/PWP reconstructions/Combined_data/",
input_from_file = FALSE, # Should input be read from a file?
object_name = dat, # Name of object with input (only if input_from_file = FALSE)
transfer_function = "KimONeil97", # Set transfer function of the record, default is Kim and O'Neil 1997.
t_int = 1, # Set time interval in days
T_per = 365, # Set annual time period in days (default = 365)
d18Ow = 0, # Set d18Ow value or vector (default = constant year-round at 0 VSMOW). Alternative options are either one value (assumed constant year-round) or a vector with length T_per / t_int and interval t_int specifying d18Ow evolution through one year.
t_maxtemp = 182.5, # Define the day of the year at which temperature is heighest. Default = Assume that the day of maximum temperature is helfway through the year
SCEUApar = c(1, 25, 10000, 5, 0.01, 0.01), # Set parameters for SCEUA optimization (iniflg, ngs, maxn, kstop, pcento, peps)
sinfit = TRUE, # Apply sinusoidal fitting to guess initial parameters for SCEUA optimization? (TRUE/FALSE)
MC = 1000, # Number of MC simulations to include measurement error into error analysis. Default = 1000 (if MC = 0, error on D and d18O measurements not considered)
plot = TRUE, # Should intermediate plots be given to track progress? WARNING: plotting makes the script much slower, especially for long datasets.
plot_export = TRUE, # Should a plot of the results be saved as PDF?
export_raw = FALSE, # Should the results of all individual model runs be exported as CSV files?
export_path = "E:/Dropbox/Research/postdoc/UNBIAS/PWP reconstructions/Combined_data/"
)
devtools::install()
require(ShellChron)
wrap_function(
path = "E:/Dropbox/Research/postdoc/UNBIAS/PWP reconstructions/Combined_data/",
input_from_file = FALSE, # Should input be read from a file?
object_name = dat, # Name of object with input (only if input_from_file = FALSE)
transfer_function = "KimONeil97", # Set transfer function of the record, default is Kim and O'Neil 1997.
t_int = 1, # Set time interval in days
T_per = 365, # Set annual time period in days (default = 365)
d18Ow = 0, # Set d18Ow value or vector (default = constant year-round at 0 VSMOW). Alternative options are either one value (assumed constant year-round) or a vector with length T_per / t_int and interval t_int specifying d18Ow evolution through one year.
t_maxtemp = 182.5, # Define the day of the year at which temperature is heighest. Default = Assume that the day of maximum temperature is helfway through the year
SCEUApar = c(1, 25, 10000, 5, 0.01, 0.01), # Set parameters for SCEUA optimization (iniflg, ngs, maxn, kstop, pcento, peps)
sinfit = TRUE, # Apply sinusoidal fitting to guess initial parameters for SCEUA optimization? (TRUE/FALSE)
MC = 1000, # Number of MC simulations to include measurement error into error analysis. Default = 1000 (if MC = 0, error on D and d18O measurements not considered)
plot = TRUE, # Should intermediate plots be given to track progress? WARNING: plotting makes the script much slower, especially for long datasets.
plot_export = TRUE, # Should a plot of the results be saved as PDF?
export_raw = FALSE, # Should the results of all individual model runs be exported as CSV files?
export_path = "E:/Dropbox/Research/postdoc/UNBIAS/PWP reconstructions/Combined_data/"
)
?wrap_function
require(ShellChron)
?wrap_function
require(tidyverse)
require(ShellChron)
All_sample_data <- read.csv("E:/Dropbox/Research/postdoc/UNBIAS/PWP reconstructions/Combined_data/PWP_sample_data.csv")
# Extract sample numbers and take average for pooled samples
All_sample_data$Sample_nr2 <- NA
for(i in 1:nrow(All_sample_data)){
if(!is.na(as.numeric(All_sample_data$Sample_nr[i]))){
# If sample is a single-number sample, just return the sample number
All_sample_data$Sample_nr2[i] <- as.numeric(All_sample_data$Sample_nr[i])
} else if(!is_empty(grep(";", All_sample_data$Sample_nr[i]))){
# Find instances of multiple sample numbers separated by semicolons
# Return the average of the pooled sample numbers
All_sample_data$Sample_nr2[i] <- mean(as.numeric(strsplit(All_sample_data$Sample_nr[i], ";")[[1]]))
} else if(!is_empty(grep("-", All_sample_data$Sample_nr[i]))){
# Find instances of a range of sample numbers separated by a dash
# Return the average of the pooled sample numbers
All_sample_data$Sample_nr2[i] <- mean(as.numeric(strsplit(All_sample_data$Sample_nr[i], "-")[[1]]))
}
}
# Apply shellchron on species to create chronology based on d18Oc
SG113 <- subset(All_sample_data, Specimen == "SG113")
SG113sg <- SG113 %>%
select(
D = Sample_nr2,
d18O = Final_d18O,
d18O_err = d18O_sd_ext
) %>%
mutate(
D_err = 0.5,
YEARMARKER = 0
)
View(SG113)
View(SG113sg)
wrap_function(
path = "E:/Dropbox/Research/postdoc/UNBIAS/PWP reconstructions/Combined_data/",
input_from_file = FALSE, # Should input be read from a file?
object_name = SG113, # Name of object with input (only if input_from_file = FALSE)
transfer_function = "KimONeil97", # Set transfer function of the record, default is Kim and O'Neil 1997.
t_int = 1, # Set time interval in days
T_per = 365, # Set annual time period in days (default = 365)
d18Ow = 0, # Set d18Ow value or vector (default = constant year-round at 0 VSMOW). Alternative options are either one value (assumed constant year-round) or a vector with length T_per / t_int and interval t_int specifying d18Ow evolution through one year.
t_maxtemp = 182.5, # Define the day of the year at which temperature is heighest. Default = Assume that the day of maximum temperature is helfway through the year
SCEUApar = c(1, 25, 10000, 5, 0.01, 0.01), # Set parameters for SCEUA optimization (iniflg, ngs, maxn, kstop, pcento, peps)
sinfit = TRUE, # Apply sinusoidal fitting to guess initial parameters for SCEUA optimization? (TRUE/FALSE)
MC = 1000, # Number of MC simulations to include measurement error into error analysis. Default = 1000 (if MC = 0, error on D and d18O measurements not considered)
plot = TRUE, # Should intermediate plots be given to track progress? WARNING: plotting makes the script much slower, especially for long datasets.
plot_export = TRUE, # Should a plot of the results be saved as PDF?
export_raw = FALSE, # Should the results of all individual model runs be exported as CSV files?
export_path = "E:/Dropbox/Research/postdoc/UNBIAS/PWP reconstructions/Combined_data/"
)
devtools::document()
devtools::install()
require(ShellChron)
wrap_function()
wrap_function
require(tidyverse)
require(ShellChron)
All_sample_data <- read.csv("E:/Dropbox/Research/postdoc/UNBIAS/PWP reconstructions/Combined_data/PWP_sample_data.csv")
# Extract sample numbers and take average for pooled samples
All_sample_data$Sample_nr2 <- NA
for(i in 1:nrow(All_sample_data)){
if(!is.na(as.numeric(All_sample_data$Sample_nr[i]))){
# If sample is a single-number sample, just return the sample number
All_sample_data$Sample_nr2[i] <- as.numeric(All_sample_data$Sample_nr[i])
} else if(!is_empty(grep(";", All_sample_data$Sample_nr[i]))){
# Find instances of multiple sample numbers separated by semicolons
# Return the average of the pooled sample numbers
All_sample_data$Sample_nr2[i] <- mean(as.numeric(strsplit(All_sample_data$Sample_nr[i], ";")[[1]]))
} else if(!is_empty(grep("-", All_sample_data$Sample_nr[i]))){
# Find instances of a range of sample numbers separated by a dash
# Return the average of the pooled sample numbers
All_sample_data$Sample_nr2[i] <- mean(as.numeric(strsplit(All_sample_data$Sample_nr[i], "-")[[1]]))
}
}
# Apply shellchron on species to create chronology based on d18Oc
SG113 <- subset(All_sample_data, Specimen == "SG113")
SG113sg <- SG113 %>%
select(
D = Sample_nr2,
d18O = Final_d18O,
d18O_err = d18O_sd_ext
) %>%
mutate(
D_err = 0.5,
YEARMARKER = 0
)
wrap_function(
path = "E:/Dropbox/Research/postdoc/UNBIAS/PWP reconstructions/Combined_data/",
input_from_file = FALSE, # Should input be read from a file?
object_name = SG113, # Name of object with input (only if input_from_file = FALSE)
transfer_function = "KimONeil97", # Set transfer function of the record, default is Kim and O'Neil 1997.
t_int = 1, # Set time interval in days
T_per = 365, # Set annual time period in days (default = 365)
d18Ow = 0, # Set d18Ow value or vector (default = constant year-round at 0 VSMOW). Alternative options are either one value (assumed constant year-round) or a vector with length T_per / t_int and interval t_int specifying d18Ow evolution through one year.
t_maxtemp = 182.5, # Define the day of the year at which temperature is heighest. Default = Assume that the day of maximum temperature is helfway through the year
SCEUApar = c(1, 25, 10000, 5, 0.01, 0.01), # Set parameters for SCEUA optimization (iniflg, ngs, maxn, kstop, pcento, peps)
sinfit = TRUE, # Apply sinusoidal fitting to guess initial parameters for SCEUA optimization? (TRUE/FALSE)
MC = 1000, # Number of MC simulations to include measurement error into error analysis. Default = 1000 (if MC = 0, error on D and d18O measurements not considered)
plot = TRUE, # Should intermediate plots be given to track progress? WARNING: plotting makes the script much slower, especially for long datasets.
plot_export = TRUE, # Should a plot of the results be saved as PDF?
export_raw = FALSE, # Should the results of all individual model runs be exported as CSV files?
export_path = "E:/Dropbox/Research/postdoc/UNBIAS/PWP reconstructions/Combined_data/"
)
?data_import_object
data_import_object(SG113)
View(SG113sg)
wrap_function(
path = "E:/Dropbox/Research/postdoc/UNBIAS/PWP reconstructions/Combined_data/",
input_from_file = FALSE, # Should input be read from a file?
object_name = SG113sg, # Name of object with input (only if input_from_file = FALSE)
transfer_function = "KimONeil97", # Set transfer function of the record, default is Kim and O'Neil 1997.
t_int = 1, # Set time interval in days
T_per = 365, # Set annual time period in days (default = 365)
d18Ow = 0, # Set d18Ow value or vector (default = constant year-round at 0 VSMOW). Alternative options are either one value (assumed constant year-round) or a vector with length T_per / t_int and interval t_int specifying d18Ow evolution through one year.
t_maxtemp = 182.5, # Define the day of the year at which temperature is heighest. Default = Assume that the day of maximum temperature is helfway through the year
SCEUApar = c(1, 25, 10000, 5, 0.01, 0.01), # Set parameters for SCEUA optimization (iniflg, ngs, maxn, kstop, pcento, peps)
sinfit = TRUE, # Apply sinusoidal fitting to guess initial parameters for SCEUA optimization? (TRUE/FALSE)
MC = 1000, # Number of MC simulations to include measurement error into error analysis. Default = 1000 (if MC = 0, error on D and d18O measurements not considered)
plot = TRUE, # Should intermediate plots be given to track progress? WARNING: plotting makes the script much slower, especially for long datasets.
plot_export = TRUE, # Should a plot of the results be saved as PDF?
export_raw = FALSE, # Should the results of all individual model runs be exported as CSV files?
export_path = "E:/Dropbox/Research/postdoc/UNBIAS/PWP reconstructions/Combined_data/"
)
data_import_object(SG113sg)
devtools::document()
devtools::install()
require(ShellChron)
data_import_object
